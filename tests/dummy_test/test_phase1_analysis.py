"""
Phase 1 í…ŒìŠ¤íŠ¸: ë‹¨ì¼/ë©€í‹° í…ìŠ¤íŠ¸ ë¶„ì„
ë°±ì—”ë“œ ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ path ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.nodes.document_analysis_node import (
    DocumentAnalysisNode,
    SourceDocument,
    create_source_from_text,
    create_sources_from_texts,
    save_analysis_to_json,
    print_analysis_summary
)


# ============================================================================
# í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒ˜í”Œ
# ============================================================================

SAMPLE_TEXT_1 = """
AI ê¸°ìˆ ì˜ ë°œì „ê³¼ ë¯¸ë˜ ì „ë§

ì¸ê³µì§€ëŠ¥(AI) ê¸°ìˆ ì€ ìµœê·¼ ëª‡ ë…„ ì‚¬ì´ ê¸‰ê²©í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë“±ì¥ìœ¼ë¡œ 
ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. 

GPT, Claude, Gemini ë“±ì˜ ëª¨ë¸ë“¤ì€ í…ìŠ¤íŠ¸ ìƒì„±, ë²ˆì—­, ìš”ì•½, ì§ˆì˜ì‘ë‹µ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë©°,
ê·¸ ì •í™•ë„ì™€ ìì—°ìŠ¤ëŸ¬ì›€ì€ ì´ì „ ì„¸ëŒ€ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.

í˜„ì¬ AI ê¸°ìˆ ì€ ì˜ë£Œ, ê¸ˆìœµ, êµìœ¡, ì œì¡°ì—… ë“± ê±°ì˜ ëª¨ë“  ì‚°ì—… ë¶„ì•¼ì— ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
íŠ¹íˆ ì˜ë£Œ ë¶„ì•¼ì—ì„œëŠ” ì§ˆë³‘ ì§„ë‹¨, ì‹ ì•½ ê°œë°œ, í™˜ì ë°ì´í„° ë¶„ì„ ë“±ì— AIê°€ í™œìš©ë˜ì–´ 
ì§„ë‹¨ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê³  ì¹˜ë£Œ íš¨ìœ¨ì„ ê°œì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ AI ê¸°ìˆ ì˜ ë°œì „ê³¼ í•¨ê»˜ ìœ¤ë¦¬ì  ë¬¸ì œë„ ëŒ€ë‘ë˜ê³  ìˆìŠµë‹ˆë‹¤.
ë°ì´í„° í”„ë¼ì´ë²„ì‹œ, ì•Œê³ ë¦¬ì¦˜ í¸í–¥ì„±, ì¼ìë¦¬ ëŒ€ì²´ ë“±ì˜ ë¬¸ì œëŠ” 
AI ê¸°ìˆ ì„ ì‚¬íšŒì— ë„ì…í•˜ëŠ” ê³¼ì •ì—ì„œ ë°˜ë“œì‹œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤ì…ë‹ˆë‹¤.

ì „ë¬¸ê°€ë“¤ì€ í–¥í›„ 5ë…„ ë‚´ì— AI ê¸°ìˆ ì´ í˜„ì¬ë³´ë‹¤ í›¨ì”¬ ë” ë°œì „í•˜ì—¬ 
AGI(Artificial General Intelligence)ì— í•œ ê±¸ìŒ ë” ë‹¤ê°€ê°ˆ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì´ëŸ¬í•œ ê¸°ìˆ  ë°œì „ì€ ì¸ë¥˜ì—ê²Œ ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì œê³µí•˜ëŠ” ë™ì‹œì— 
ìƒˆë¡œìš´ ë„ì „ê³¼ì œë¥¼ ë˜ì§ˆ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
"""

SAMPLE_TEXT_2 = """
ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì´í•´

ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì§€ ì•Šê³ ë„ 
ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ íŒ¨í„´ì„ ì°¾ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

ë¨¸ì‹ ëŸ¬ë‹ì˜ ì£¼ìš” í•™ìŠµ ë°©ì‹ì€ ì„¸ ê°€ì§€ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:
1. ì§€ë„ í•™ìŠµ(Supervised Learning): ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ë¡œ í•™ìŠµ
2. ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning): ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ ë°œê²¬
3. ê°•í™” í•™ìŠµ(Reinforcement Learning): ë³´ìƒì„ í†µí•œ ì‹œí–‰ì°©ì˜¤ í•™ìŠµ

ë”¥ëŸ¬ë‹(Deep Learning)ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ë¡œ, ì¸ê³µ ì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì•„
ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. íŠ¹íˆ ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬ ë“±ì˜
ë¶„ì•¼ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.

ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ êµ¬ì¡°ì¸ ì‹ ê²½ë§ì€ ì¸ê°„ì˜ ë‡Œ êµ¬ì¡°ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.
ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ì¸µì˜ ë‰´ëŸ°ë“¤ì´ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µí•´
ì •ë³´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ìµœê·¼ì—ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ì•„í‚¤í…ì²˜ê°€ ë“±ì¥í•˜ë©´ì„œ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì— 
í˜ëª…ì ì¸ ë³€í™”ê°€ ì¼ì–´ë‚¬ìŠµë‹ˆë‹¤. BERT, GPT ì‹œë¦¬ì¦ˆ ë“±ì´ ëª¨ë‘ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì´ë©°,
ì–´í…ì…˜(Attention) ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë¬¸ë§¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.

ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì€ ê³„ì† ë°œì „í•˜ê³  ìˆìœ¼ë©°, ì•ìœ¼ë¡œë„ ë” ë§ì€ í˜ì‹ ì´ 
ê¸°ëŒ€ë˜ëŠ” ë¶„ì•¼ì…ë‹ˆë‹¤.
"""

SAMPLE_TEXT_3 = """
AI ìœ¤ë¦¬ì™€ ê·œì œ

AI ê¸°ìˆ ì´ ì‚¬íšŒ ì „ë°˜ì— ë¹ ë¥´ê²Œ í™•ì‚°ë˜ë©´ì„œ ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­ê³¼ ê·œì œì˜ í•„ìš”ì„±ì´ 
ì ì  ë” ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì£¼ìš” AI ìœ¤ë¦¬ ì´ìŠˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ë°ì´í„° í”„ë¼ì´ë²„ì‹œ
ê°œì¸ì •ë³´ ë³´í˜¸ëŠ” AI ì‹œìŠ¤í…œ ê°œë°œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê³ ë ¤ì‚¬í•­ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
AI ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ëŒ€ëŸ‰ì˜ ë°ì´í„°ê°€ í•„ìš”í•œë°, 
ì´ ê³¼ì •ì—ì„œ ê°œì¸ì •ë³´ê°€ ë¬´ë‹¨ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê±°ë‚˜ ì˜¤ìš©ë  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.

2. ì•Œê³ ë¦¬ì¦˜ í¸í–¥ì„±
AI ì‹œìŠ¤í…œì€ í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥ì„ ê·¸ëŒ€ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì±„ìš© AIê°€ íŠ¹ì • ì„±ë³„ì´ë‚˜ ì¸ì¢…ì„ ì°¨ë³„í•˜ëŠ” ê²°ê³¼ë¥¼ ë‚³ì„ ìˆ˜ ìˆìœ¼ë©°,
ì´ëŠ” ì‚¬íšŒì  ë¶ˆí‰ë“±ì„ ì‹¬í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. íˆ¬ëª…ì„±ê³¼ ì„¤ëª…ê°€ëŠ¥ì„±
ë§ì€ AI ëª¨ë¸, íŠ¹íˆ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ "ë¸”ë™ë°•ìŠ¤"ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
ì™œ ê·¸ëŸ¬í•œ ê²°ì •ì„ ë‚´ë ¸ëŠ”ì§€ ì„¤ëª…í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ê°€ ë§ì•„,
ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì •ì— AIë¥¼ ì‚¬ìš©í•  ë•Œ ì‹ ë¢°ì„± ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

4. ì±…ì„ê³¼ ì±…ë¬´
AI ì‹œìŠ¤í…œì´ ì˜ëª»ëœ ê²°ì •ì„ ë‚´ë ¸ì„ ë•Œ ëˆ„ê°€ ì±…ì„ì„ ì ¸ì•¼ í•˜ëŠ”ê°€?
ê°œë°œì, ì‚¬ìš©ì, ê¸°ì—…, ì•„ë‹ˆë©´ AI ìì²´ì¸ê°€? 
ì´ëŠ” ë²•ì , ìœ¤ë¦¬ì ìœ¼ë¡œ ë³µì¡í•œ ë¬¸ì œì…ë‹ˆë‹¤.

ê°êµ­ ì •ë¶€ì™€ êµ­ì œê¸°êµ¬ë“¤ì€ AI ê·œì œ í”„ë ˆì„ì›Œí¬ë¥¼ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.
ìœ ëŸ½ì—°í•©ì˜ AI Act, ë¯¸êµ­ì˜ AI ê¶Œë¦¬ì¥ì „, í•œêµ­ì˜ AI ìœ¤ë¦¬ê¸°ì¤€ ë“±ì´ 
ëŒ€í‘œì ì¸ ì˜ˆì…ë‹ˆë‹¤.

AI ìœ¤ë¦¬ëŠ” ê¸°ìˆ  ë°œì „ë§Œí¼ì´ë‚˜ ì¤‘ìš”í•˜ë©°, ì§€ì†ê°€ëŠ¥í•œ AI ë°œì „ì„ ìœ„í•´ì„œëŠ”
ê¸°ìˆ ê³¼ ìœ¤ë¦¬ê°€ í•¨ê»˜ ë°œì „í•´ì•¼ í•©ë‹ˆë‹¤.
"""


# ============================================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ============================================================================

def test_single_document():
    """í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„"""
    print("\n" + "="*80)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„")
    print("="*80)
    
    # ë¶„ì„ ë…¸ë“œ ì´ˆê¸°í™”
    analyzer = DocumentAnalysisNode(model_name="gemini-2.5-flash")
    
    # ë‹¨ì¼ ë¬¸ì„œ ìƒì„±
    source = create_source_from_text(SAMPLE_TEXT_1, "ai_technology_overview")
    
    # ë¶„ì„ ì‹¤í–‰
    result = analyzer.analyze_documents([source])
    
    # ê²°ê³¼ ì¶œë ¥
    print_analysis_summary(result)
    
    # ì›ë³¸ ì¶œë ¥ í™•ì¸
    print("\n" + "="*80)
    print("ğŸ“„ Gemini ì›ë³¸ ì¶œë ¥")
    print("="*80)
    print(result.metadata['raw_output'])
    
    # JSON ì €ì¥
    save_analysis_to_json(result, "./test_output/single_doc_analysis.json")
    
    return result


def test_multi_documents():
    """í…ŒìŠ¤íŠ¸ 2: ë©€í‹° ë¬¸ì„œ ë¶„ì„ (3ê°œ)"""
    print("\n" + "="*80)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 2: ë©€í‹° ë¬¸ì„œ ë¶„ì„ (3ê°œ ë¬¸ì„œ)")
    print("="*80)
    
    # ë¶„ì„ ë…¸ë“œ ì´ˆê¸°í™”
    analyzer = DocumentAnalysisNode(model_name="gemini-2.5-flash")
    
    # ì—¬ëŸ¬ ë¬¸ì„œ ìƒì„±
    sources = [
        SourceDocument(id="doc_1_ai_overview", content=SAMPLE_TEXT_1, doc_type="text"),
        SourceDocument(id="doc_2_ml_dl", content=SAMPLE_TEXT_2, doc_type="text"),
        SourceDocument(id="doc_3_ai_ethics", content=SAMPLE_TEXT_3, doc_type="text"),
    ]
    
    # ë¶„ì„ ì‹¤í–‰
    result = analyzer.analyze_documents(sources)
    
    # ê²°ê³¼ ì¶œë ¥
    print_analysis_summary(result)
    
    # ì›ë³¸ ì¶œë ¥ í™•ì¸
    print("\n" + "="*80)
    print("ğŸ“„ Gemini ì›ë³¸ ì¶œë ¥")
    print("="*80)
    print(result.metadata['raw_output'])
    
    # JSON ì €ì¥
    save_analysis_to_json(result, "./test_output/multi_doc_analysis.json")
    
    return result


def test_custom_text():
    """í…ŒìŠ¤íŠ¸ 3: ì‚¬ìš©ì ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ì…ë ¥"""
    print("\n" + "="*80)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 3: ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ì…ë ¥")
    print("="*80)
    
    print("\ní…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì™„ë£Œí•˜ë ¤ë©´ ë¹ˆ ì¤„ì—ì„œ Ctrl+D ë˜ëŠ” Ctrl+Z):")
    print("-" * 80)
    
    lines = []
    try:
        while True:
            line = input()
            lines.append(line)
    except EOFError:
        pass
    
    custom_text = "\n".join(lines)
    
    if not custom_text.strip():
        print("âš ï¸  í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        custom_text = SAMPLE_TEXT_1
    
    # ë¶„ì„ ë…¸ë“œ ì´ˆê¸°í™”
    analyzer = DocumentAnalysisNode(model_name="gemini-2.5-flash")
    
    # ë¬¸ì„œ ìƒì„±
    source = create_source_from_text(custom_text, "custom_input")
    
    # ë¶„ì„ ì‹¤í–‰
    result = analyzer.analyze_documents([source])
    
    # ê²°ê³¼ ì¶œë ¥
    print_analysis_summary(result)
    
    # ì›ë³¸ ì¶œë ¥ í™•ì¸
    print("\n" + "="*80)
    print("ğŸ“„ Gemini ì›ë³¸ ì¶œë ¥")
    print("="*80)
    print(result.metadata['raw_output'])
    
    # JSON ì €ì¥
    save_analysis_to_json(result, "./test_output/custom_text_analysis.json")
    
    return result


def test_langgraph_node():
    """í…ŒìŠ¤íŠ¸ 4: LangGraph ë…¸ë“œë¡œ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 4: LangGraph ë…¸ë“œ í˜•ì‹ìœ¼ë¡œ ì‹¤í–‰")
    print("="*80)
    
    # ë¶„ì„ ë…¸ë“œ ì´ˆê¸°í™”
    analyzer = DocumentAnalysisNode(model_name="gemini-2.5-flash")
    
    # State ì¤€ë¹„
    state = {
        "sources": [
            SourceDocument(id="doc_1", content=SAMPLE_TEXT_1, doc_type="text"),
            SourceDocument(id="doc_2", content=SAMPLE_TEXT_2, doc_type="text"),
        ]
    }
    
    # ë…¸ë“œ ì‹¤í–‰ (__call__ ë©”ì„œë“œ)
    result_state = analyzer(state)
    
    # ê²°ê³¼ í™•ì¸
    print("âœ… LangGraph ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ")
    print(f"State keys: {list(result_state.keys())}")
    
    analysis_result = result_state['analysis_result']
    print_analysis_summary(analysis_result)
    
    # ì›ë³¸ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“„ Gemini ì›ë³¸ ì¶œë ¥")
    print("="*80)
    print(analysis_result.metadata['raw_output'])
    
    return result_state


def test_edge_cases():
    """í…ŒìŠ¤íŠ¸ 5: ì—£ì§€ ì¼€ì´ìŠ¤"""
    print("\n" + "="*80)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 5: ì—£ì§€ ì¼€ì´ìŠ¤")
    print("="*80)
    
    analyzer = DocumentAnalysisNode(model_name="gemini-2.5-flash")
    
    # ì¼€ì´ìŠ¤ 1: ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸
    print("\n--- ì¼€ì´ìŠ¤ 1: ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ ---")
    short_source = create_source_from_text(
        "AIëŠ” ë¯¸ë˜ë‹¤.", 
        "very_short"
    )
    result1 = analyzer.analyze_documents([short_source])
    print(f"âœ… ì™„ë£Œ (ì¶œë ¥ ê¸¸ì´: {len(result1.metadata['raw_output'])})")
    
    # ì¼€ì´ìŠ¤ 2: ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
    print("\n--- ì¼€ì´ìŠ¤ 2: ê¸´ í…ìŠ¤íŠ¸ (ë°˜ë³µ) ---")
    long_text = SAMPLE_TEXT_1 + "\n\n" + SAMPLE_TEXT_2 + "\n\n" + SAMPLE_TEXT_3
    long_text = long_text * 3  # 3ë°° ë°˜ë³µ
    long_source = create_source_from_text(long_text, "very_long")
    result2 = analyzer.analyze_documents([long_source])
    print(f"âœ… ì™„ë£Œ (ì¶œë ¥ ê¸¸ì´: {len(result2.metadata['raw_output'])})")
    
    # ì¼€ì´ìŠ¤ 3: ë§ì€ ë¬¸ì„œ (10ê°œ)
    print("\n--- ì¼€ì´ìŠ¤ 3: ë§ì€ ë¬¸ì„œ (10ê°œ) ---")
    many_sources = create_sources_from_texts([
        SAMPLE_TEXT_1, SAMPLE_TEXT_2, SAMPLE_TEXT_3,
        SAMPLE_TEXT_1, SAMPLE_TEXT_2, SAMPLE_TEXT_3,
        SAMPLE_TEXT_1, SAMPLE_TEXT_2, SAMPLE_TEXT_3,
        SAMPLE_TEXT_1
    ])
    result3 = analyzer.analyze_documents(many_sources)
    print(f"âœ… ì™„ë£Œ (ì¶œë ¥ ê¸¸ì´: {len(result3.metadata['raw_output'])})")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("="*80)
    print("ğŸš€ Phase 1: ë‹¨ì¼/ë©€í‹° í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs("./test_output", exist_ok=True)
    
    print("\ní…ŒìŠ¤íŠ¸ ì„ íƒ:")
    print("1. ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„")
    print("2. ë©€í‹° ë¬¸ì„œ ë¶„ì„ (3ê°œ)")
    print("3. ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ì…ë ¥")
    print("4. LangGraph ë…¸ë“œ í˜•ì‹")
    print("5. ì—£ì§€ ì¼€ì´ìŠ¤")
    print("6. ì „ì²´ í…ŒìŠ¤íŠ¸")
    
    choice = input("\në²ˆí˜¸ ì…ë ¥ (1-6): ").strip()
    
    try:
        if choice == "1":
            test_single_document()
        elif choice == "2":
            test_multi_documents()
        elif choice == "3":
            test_custom_text()
        elif choice == "4":
            test_langgraph_node()
        elif choice == "5":
            test_edge_cases()
        elif choice == "6":
            print("\nğŸš€ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n")
            test_single_document()
            input("\nê³„ì†í•˜ë ¤ë©´ Enter...")
            test_multi_documents()
            input("\nê³„ì†í•˜ë ¤ë©´ Enter...")
            test_langgraph_node()
            input("\nê³„ì†í•˜ë ¤ë©´ Enter...")
            test_edge_cases()
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*80)
    print("âœ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ“ ì¶œë ¥ íŒŒì¼: ./test_output/")
    print("="*80)


if __name__ == "__main__":
    # Vertex AI ì´ˆê¸°í™” (credentials í•„ìš”)
    import vertexai
    
    vertexai.init(
        project="alan-document-lab",
        location="us-central1"
    )
    
    main()
