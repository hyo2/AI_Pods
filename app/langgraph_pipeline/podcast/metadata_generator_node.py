"""
Metadata Generator Node
=======================

ì…ë ¥:
- main_file: ì£¼ê°•ì˜ìë£Œ (1ê°œ, í•„ìˆ˜)
- aux_files: ë³´ì¡°ìë£Œ (0~3ê°œ, ì„ íƒ)

ì¶œë ¥:
- metadata.json (ì´ë¯¸ì§€ ì„¤ëª… í¬í•¨, íŒŒì¼ ì €ì¥ ì•ˆ í•¨)

í†µí•©:
- DocumentConverterNode: PDF ë³€í™˜
- ImprovedHybridFilterPipeline: ì´ë¯¸ì§€ í•„í„°ë§
- TextExtractor: í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ImageDescriptionGenerator: ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª…
"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
import pdfplumber

# ê¸°ì¡´ ë…¸ë“œ ì„í¬íŠ¸
from .document_converter_node import DocumentConverterNode
from .improved_hybrid_filter import (
    ImprovedHybridFilterPipeline,
    UniversalImageExtractor,
    ImageMetadata,
    model
)

from vertexai.generative_models import Part


class TextExtractor:
    """PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…"""
    
    def extract_with_markers(
        self, 
        pdf_path: str, 
        prefix: str = "MAIN"
    ) -> Dict[str, Any]:
        """
        PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë§ˆì»¤ ì‚½ì…
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            prefix: í˜ì´ì§€ ë§ˆì»¤ ì ‘ë‘ì‚¬ (MAIN, SUPP1, SUPP2, SUPP3)
        
        Returns:
            {
                "full_text": "[MAIN-PAGE 1: ì œëª©]\në‚´ìš©...",
                "total_pages": 21
            }
        """
        pages_text = []
        total_pages = 0
        
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            
            for page_num, page in enumerate(pdf.pages, 1):
                # í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = page.extract_text() or ""
                
                # í˜ì´ì§€ ì œëª© ì¶”ì¶œ (ì²« ì¤„ ë˜ëŠ” ì²˜ìŒ 50ì)
                lines = text.split('\n')
                title = lines[0][:50] if lines and lines[0].strip() else f"Page {page_num}"
                
                # í˜ì´ì§€ ë§ˆì»¤ + ë‚´ìš©
                pages_text.append(f"[{prefix}-PAGE {page_num}: {title}]")
                pages_text.append(text)
                pages_text.append("")  # í˜ì´ì§€ ê°„ êµ¬ë¶„
        
        return {
            "full_text": "\n".join(pages_text),
            "total_pages": total_pages
        }


class ImageDescriptionGenerator:
    """í†µê³¼ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„± (2-4ë¬¸ì¥)"""
    
    def generate_description(
        self, 
        image_bytes: bytes, 
        adjacent_text: str,
        keywords: List[str]
    ) -> str:
        """
        Vision APIë¡œ ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… ìƒì„±
        
        Args:
            image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
            adjacent_text: ì£¼ë³€ í…ìŠ¤íŠ¸
            keywords: ë¬¸ì„œ í‚¤ì›Œë“œ
        
        Returns:
            2-4ë¬¸ì¥ì˜ ìƒì„¸ ì„¤ëª…
        """
        try:
            # MIME íƒ€ì… ê°ì§€
            mime_type = self._get_mime_type(image_bytes)
            image_part = Part.from_data(data=image_bytes, mime_type=mime_type)
            
            keyword_context = ', '.join(keywords[:10]) if keywords else "ì¼ë°˜ í•™ìŠµ ë‚´ìš©"
            
            prompt = f"""
ì´ ì´ë¯¸ì§€ë¥¼ 2-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ê°•ì˜ ì£¼ì œ: {keyword_context}
ì£¼ë³€ í…ìŠ¤íŠ¸: "{adjacent_text}"

ì„¤ëª…ì— í¬í•¨í•  ë‚´ìš©:
1. ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚´ëŠ” ì£¼ì œ/ê°œë… (1ë¬¸ì¥)
2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ 2-3ê°œ (1-2ë¬¸ì¥)
3. í•µì‹¬ ì •ë³´ë‚˜ íŒ¨í„´ (1ë¬¸ì¥)

ì œì™¸í•  ë‚´ìš©:
- ì„¸ë¶€ ìš”ì†Œ ì „ì²´ ë‚˜ì—´
- ë¶ˆí•„ìš”í•œ ì¶”ì¸¡ì´ë‚˜ í•´ì„

ì¶œë ¥: ëª…í™•í•˜ê³  ê°„ê²°í•œ 2-4ë¬¸ì¥ë§Œ.
"""
            
            response = model.generate_content([image_part, prompt])
            return response.text.strip()
            
        except Exception as e:
            return f"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def _get_mime_type(self, image_bytes: bytes) -> str:
        """ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ì—ì„œ MIME íƒ€ì… ê°ì§€"""
        if image_bytes.startswith(b'\xff\xd8'):
            return "image/jpeg"
        elif image_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return "image/png"
        elif image_bytes.startswith(b'GIF87a') or image_bytes.startswith(b'GIF89a'):
            return "image/gif"
        elif image_bytes.startswith(b'RIFF') and image_bytes[8:12] == b'WEBP':
            return "image/webp"
        return "image/png"


class MetadataGenerator:
    """
    ë©”íƒ€ë°ì´í„° ìƒì„± ë…¸ë“œ
    
    ì£¼ê°•ì˜ìë£Œ + ë³´ì¡°ìë£Œ â†’ metadata.json
    """
    
    def __init__(self):
        self.converter = None
        self.text_extractor = TextExtractor()
        self.image_filter = ImprovedHybridFilterPipeline(auto_extract_keywords=True)
        self.image_describer = ImageDescriptionGenerator()
    
    def _extract_page_title(self, slide_title: str, adjacent_text: str) -> str:
        """
        ì˜ë¯¸ìˆëŠ” í˜ì´ì§€ ì œëª© ì¶”ì¶œ
        
        1ìˆœìœ„: slide.title (ìˆê³  ì˜ë¯¸ìˆìœ¼ë©´)
        2ìˆœìœ„: adjacent_text ì²« ì¤„
        3ìˆœìœ„: "í˜ì´ì§€ ì œëª© ì—†ìŒ"
        
        Args:
            slide_title: PPTXì˜ slide.title
            adjacent_text: ìŠ¬ë¼ì´ë“œ ì „ì²´ í…ìŠ¤íŠ¸
        
        Returns:
            ì¶”ì¶œëœ í˜ì´ì§€ ì œëª© (ìµœëŒ€ 50ì)
        """
        # 1. slide.titleì´ ìˆê³  ì˜ë¯¸ìˆìœ¼ë©´
        if slide_title and slide_title.strip() and slide_title.lower() != "no title":
            return slide_title.strip()[:50]
        
        # 2. adjacent_textì—ì„œ ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì¤„ ì¶”ì¶œ
        if adjacent_text:
            lines = adjacent_text.strip().split('\n')
            for line in lines:
                line = line.strip()
                # ì˜ë¯¸ìˆëŠ” ì¤„: 3ì ì´ìƒ, â˜ë¡œ ì‹œì‘ ì•ˆ í•¨, ë„ˆë¬´ ì§§ì§€ ì•ŠìŒ
                if len(line) > 3 and not line.startswith('â˜'):
                    return line[:50]
        
        # 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´
        return "í˜ì´ì§€ ì œëª© ì—†ìŒ"
    
    def generate(
        self,
        main_file: str,
        aux_files: Optional[List[str]] = None,
        output_path: str = "output/metadata.json"
    ) -> str:
        """
        ë©”íƒ€ë°ì´í„° ìƒì„±
        
        Args:
            main_file: ì£¼ê°•ì˜ìë£Œ ê²½ë¡œ
            aux_files: ë³´ì¡°ìë£Œ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (0~3ê°œ)
            output_path: ì¶œë ¥ JSON ê²½ë¡œ
        
        Returns:
            ìƒì„±ëœ metadata.json ê²½ë¡œ
        """
        print(f"\n{'='*120}")
        print(f"ğŸ¯ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"{'='*120}")
        print(f"ì£¼ê°•ì˜ìë£Œ: {main_file}")
        if aux_files:
            print(f"ë³´ì¡°ìë£Œ: {len(aux_files)}ê°œ")
            for i, supp in enumerate(aux_files, 1):
                print(f"  {i}. {supp}")
        print(f"{'='*120}\n")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        with tempfile.TemporaryDirectory() as temp_dir:
            self.converter = DocumentConverterNode(output_dir=temp_dir)
            
            # 1. ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬
            print("ğŸ“„ [1/3] ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬ ì¤‘...")
            main_metadata = self._process_main_source(main_file)
            
            # 2. ë³´ì¡°ìë£Œ ì²˜ë¦¬
            print("\nğŸ“š [2/3] ë³´ì¡°ìë£Œ ì²˜ë¦¬ ì¤‘...")
            aux_metadata = []
            if aux_files:
                for i, supp_file in enumerate(aux_files[:3], 1):  # ìµœëŒ€ 3ê°œ
                    supp_meta = self._process_aux_source(supp_file, i)
                    aux_metadata.append(supp_meta)
            else:
                print("   âš ï¸  ë³´ì¡°ìë£Œ ì—†ìŒ (ì„ íƒ ì‚¬í•­)")
            
            # 3. ìµœì¢… ë©”íƒ€ë°ì´í„° êµ¬ì„±
            print("\nğŸ”§ [3/3] ë©”íƒ€ë°ì´í„° í†µí•© ì¤‘...")
            metadata = {
                "metadata_version": "1.0",
                "created_at": datetime.now().isoformat(),
                "main_source": main_metadata,
                "aux_sources": aux_metadata
            }
            
            # 4. JSON ì €ì¥
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"\n{'='*120}")
            print(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            print(f"{'='*120}")
            print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
            print(f"ğŸ“Š ì£¼ê°•ì˜ìë£Œ í˜ì´ì§€: {main_metadata['total_pages']}ê°œ")
            print(f"ğŸ–¼ï¸  í•„í„°ë§ëœ ì´ë¯¸ì§€: {len(main_metadata['filtered_images'])}ê°œ")
            if aux_metadata:
                total_supp_pages = sum(s['total_pages'] for s in aux_metadata)
                print(f"ğŸ“š ë³´ì¡°ìë£Œ í˜ì´ì§€: {total_supp_pages}ê°œ")
            print(f"{'='*120}\n")
            
            return str(output_path)
    
    def _process_main_source(self, file_path: str) -> Dict[str, Any]:
        """
        ì£¼ê°•ì˜ìë£Œ ì²˜ë¦¬
        - PDF ë³€í™˜
        - í…ìŠ¤íŠ¸ ì¶”ì¶œ
        - ì´ë¯¸ì§€ í•„í„°ë§
        - ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        """
        file_path = Path(file_path)
        file_type = file_path.suffix.lower().replace('.', '')
        
        print(f"   ğŸ“„ íŒŒì¼: {file_path.name} ({file_type})")
        
        # 1. PDF ë³€í™˜ (í…ìŠ¤íŠ¸ ì¶”ì¶œìš©)
        print(f"   ğŸ”„ PDF ë³€í™˜ ì¤‘... (í…ìŠ¤íŠ¸ ì¶”ì¶œìš©)")
        pdf_path = self.converter.convert(str(file_path))
        
        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (í˜ì´ì§€ ë§ˆì»¤ í¬í•¨)
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text_data = self.text_extractor.extract_with_markers(pdf_path, prefix="MAIN")
        
        # 3. ì´ë¯¸ì§€ í•„í„°ë§ (í˜•ì‹ë³„ ì²˜ë¦¬)
        print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ í•„í„°ë§ ì¤‘...")
        
        if file_type == 'pptx':
            # âœ… PPTX: ì›ë³¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ (í’ˆì§ˆ ìµœìƒ)
            print(f"      â†’ PPTX ì›ë³¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            self.image_filter.extract_keywords_from_document(str(file_path))
            keywords = self.image_filter.document_keywords
            
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (python-pptx)
            all_images = self._extract_images_from_pptx(str(file_path))
            
        elif file_type in ['docx', 'pdf']:
            # âœ… DOCX/PDF: PDFì—ì„œ ì¶”ì¶œ
            print(f"      â†’ PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (pdfplumber + pdf2image)")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ë³€í™˜ëœ PDF ì‚¬ìš©)
            self.image_filter.extract_keywords_from_document(pdf_path)
            keywords = self.image_filter.document_keywords
            
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (PDF)
            extractor = UniversalImageExtractor()
            all_images = extractor.extract(pdf_path)
        
        else:
            print(f"   âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {file_type}")
            all_images = []
            keywords = []
        
        # 4. í•„í„°ë§ ì‹¤í–‰
        filtered_images = []
        if all_images:
            print(f"   ğŸ” {len(all_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬, í•„í„°ë§ ì‹œì‘...")
            
            for img_meta in all_images:
                decision, reason = self.image_filter.step1_rule_check(img_meta)
                
                if decision == "INCLUDE":
                    img_meta.is_core_content = True
                    img_meta.filter_reason = reason
                    filtered_images.append(img_meta)
                    
                elif decision == "PENDING":
                    ai_result = self.image_filter.step2_gemini_check(img_meta)
                    if ai_result.upper().startswith("KEEP"):
                        img_meta.is_core_content = True
                        img_meta.filter_reason = ai_result
                        filtered_images.append(img_meta)
            
            print(f"   âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_images)}ê°œ ì„ íƒ ({len(all_images) - len(filtered_images)}ê°œ ì œì™¸)")
        
        # 5. í†µê³¼ëœ ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… ìƒì„±
        filtered_image_metadata = []
        if filtered_images:
            print(f"   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... (0/{len(filtered_images)})", end='', flush=True)
            
            for i, img_meta in enumerate(filtered_images, 1):
                # ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
                description = self.image_describer.generate_description(
                    img_meta.image_bytes,
                    img_meta.adjacent_text,
                    keywords
                )
                
                # í˜ì´ì§€ ì œëª© ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
                page_title = self._extract_page_title(
                    img_meta.slide_title,
                    img_meta.adjacent_text
                )
                
                filtered_image_metadata.append({
                    "image_id": img_meta.image_id.replace("S", "MAIN_P").replace("P", "MAIN_P"),  # S02 or P02 â†’ MAIN_P02
                    "page_number": img_meta.slide_number,
                    "page_title": page_title,
                    "description": description,
                    "filter_stage": "1ì°¨ (Rule)" if "Rule" in img_meta.filter_reason else "2ì°¨ (AI)",
                    "area_percentage": img_meta.area_percentage
                })
                
                print(f"\r   ğŸ“ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘... ({i}/{len(filtered_images)})", end='', flush=True)
            
            print()  # ì¤„ë°”ê¿ˆ
        
        # 6. í†µê³„ ìƒì„±
        total_images = len(all_images)
        passed_images = len(filtered_images)
        
        return {
            "role": "main",
            "filename": file_path.name,
            "file_type": file_type,
            "total_pages": text_data['total_pages'],
            "content": {
                "full_text": text_data['full_text']
            },
            "filtered_images": filtered_image_metadata,
            "statistics": {
                "total_images_found": total_images,
                "images_passed": passed_images,
                "filter_rate": passed_images / total_images if total_images > 0 else 0
            }
        }
    
    def _process_aux_source(self, file_path: str, order: int) -> Dict[str, Any]:
        """
        ë³´ì¡°ìë£Œ ì²˜ë¦¬
        - PDF ë³€í™˜
        - í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì´ë¯¸ì§€ ë¬´ì‹œ)
        """
        file_path = Path(file_path)
        file_type = file_path.suffix.lower().replace('.', '')
        
        print(f"   ğŸ“š ë³´ì¡°ìë£Œ {order}: {file_path.name} ({file_type})")
        
        # 1. PDF ë³€í™˜
        print(f"      ğŸ”„ PDF ë³€í™˜ ì¤‘...")
        pdf_path = self.converter.convert(str(file_path))
        
        # 2. í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        print(f"      ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text_data = self.text_extractor.extract_with_markers(pdf_path, prefix=f"SUPP{order}")
        
        print(f"      âœ… ì™„ë£Œ ({text_data['total_pages']}í˜ì´ì§€)")
        
        return {
            "order": order,
            "filename": file_path.name,
            "file_type": file_type,
            "total_pages": text_data['total_pages'],
            "content": {
                "full_text": text_data['full_text']
            }
        }
    
    def _extract_images_from_pptx(self, pptx_path: str) -> List[ImageMetadata]:
        """PPTXì—ì„œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (UniversalImageExtractor ì‚¬ìš©)"""
        extractor = UniversalImageExtractor()
        return extractor.extract(pptx_path)


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import sys
    
    print("\n" + "="*120)
    print("ğŸ¯ Metadata Generator Node")
    print("="*120)
    
    # ì‚¬ìš©ë²•
    if len(sys.argv) < 2:
        print("\nì‚¬ìš©ë²•:")
        print("  python metadata_generator_node.py <ì£¼ê°•ì˜ìë£Œ> [ë³´ì¡°1] [ë³´ì¡°2] [ë³´ì¡°3]")
        print("\nì˜ˆì‹œ:")
        print("  # ì£¼ìë£Œë§Œ")
        print("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx")
        print("\n  # ì£¼ìë£Œ + ë³´ì¡° 1ê°œ")
        print("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx ë³´ì¡°ìë£Œ.docx")
        print("\n  # ì£¼ìë£Œ + ë³´ì¡° 3ê°œ (ìµœëŒ€)")
        print("  python metadata_generator_node.py ì¤‘ë“±êµ­ì–´1.pptx ë³´ì¡°1.docx ë³´ì¡°2.pdf ë³´ì¡°3.docx")
        print("\nâœ… ì§€ì› í˜•ì‹: PPTX, DOCX, PDF")
        print("="*120 + "\n")
        sys.exit(1)
    
    # íŒŒì¼ ê²½ë¡œ íŒŒì‹±
    main_file = sys.argv[1]
    aux_files = sys.argv[2:5] if len(sys.argv) > 2 else None  # ìµœëŒ€ 3ê°œ
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(main_file):
        print(f"\nâŒ ì£¼ê°•ì˜ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {main_file}")
        sys.exit(1)
    
    if aux_files:
        for supp in aux_files:
            if not os.path.exists(supp):
                print(f"\nâŒ ë³´ì¡°ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {supp}")
                sys.exit(1)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    try:
        generator = MetadataGenerator()
        output_path = generator.generate(
            main_file=main_file,
            aux_files=aux_files,
            output_path="output/metadata.json"
        )
        
        print(f"âœ… ì„±ê³µ!")
        print(f"ğŸ“ {output_path}")
        
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)