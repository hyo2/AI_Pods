"""
Phase 2 íŒŒì´í”„ë¼ì¸: í…ìŠ¤íŠ¸ â†’ ë¶„ì„ â†’ í† í”½ â†’ ì´ë¯¸ì§€
"""

from typing import List, Dict, Any, Optional
from dataclasses import asdict
import os
import json

from app.nodes.document_analysis_node import (
    DocumentAnalysisNode,
    SourceDocument,
    CompleteAnalysis
)
from app.nodes.topic_extraction_node import (
    TopicExtractionNode,
    ImageTopic
)
from app.nodes.image_generation_node import (
    ImageGenerationNode,
    GeneratedImage
)


class DocumentToImagePipeline:
    """ë¬¸ì„œì—ì„œ ì´ë¯¸ì§€ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        output_dir: str = "./pipeline_output",
        analysis_model: str = "gemini-2.5-flash",
        topic_model: str = "gemini-2.5-flash",
        image_default_method: str = "gemini",
        credentials_path: str = "./vertex-ai-service-account.json"
    ):
        """
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            analysis_model: ë¶„ì„ìš© Gemini ëª¨ë¸
            topic_model: í† í”½ ì¶”ì¶œìš© Gemini ëª¨ë¸
            image_default_method: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± ë°©ë²•
            credentials_path: Vertex AI credentials
        """
        self.output_dir = output_dir
        self.credentials_path = credentials_path
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "json"), exist_ok=True)
        
        # ë…¸ë“œ ì´ˆê¸°í™”
        print("ğŸ”§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        
        self.analysis_node = DocumentAnalysisNode(model_name=analysis_model)
        print("  âœ… ë¶„ì„ ë…¸ë“œ")
        
        self.topic_node = TopicExtractionNode(model_name=topic_model)
        print("  âœ… í† í”½ ì¶”ì¶œ ë…¸ë“œ")
        
        # ImagenService ì´ˆê¸°í™” (ê¸°ì¡´ ì„œë¹„ìŠ¤ ì‚¬ìš©)
        try:
            from app.services.imagen_service import ImagenService
            # ê¸°ì¡´ ImagenServiceëŠ” default_model íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
            imagen_service = ImagenService(
                project_id="alan-document-lab",
                credentials_path=credentials_path
            )
        except Exception as e:
            print(f"  âš ï¸  ImagenService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"  â„¹ï¸  ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì† ì§„í–‰...")
            imagen_service = None
        
        self.image_node = ImageGenerationNode(
            imagen_service=imagen_service,
            output_dir=os.path.join(output_dir, "images"),
            default_method=image_default_method
        )
        print("  âœ… ì´ë¯¸ì§€ ìƒì„± ë…¸ë“œ")
        
        print("âœ¨ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ\n")
    
    def run(
        self,
        sources: List[SourceDocument],
        min_topics: int = 5,
        max_topics: int = 20,
        generation_strategy: str = "auto",
        save_intermediate: bool = True
    ) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            sources: ì…ë ¥ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            min_topics: ìµœì†Œ í† í”½ ê°œìˆ˜
            max_topics: ìµœëŒ€ í† í”½ ê°œìˆ˜
            generation_strategy: ì´ë¯¸ì§€ ìƒì„± ì „ëµ
            save_intermediate: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        
        Returns:
            {
                "analysis": CompleteAnalysis,
                "topics": List[ImageTopic],
                "images": List[GeneratedImage],
                "paths": {
                    "analysis_json": str,
                    "topics_json": str,
                    "images_json": str,
                    "gallery_html": str
                }
            }
        """
        print("="*80)
        print("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*80)
        print(f"\nì…ë ¥ ë¬¸ì„œ: {len(sources)}ê°œ")
        print(f"í† í”½ ë²”ìœ„: {min_topics} ~ {max_topics}ê°œ")
        print(f"ìƒì„± ì „ëµ: {generation_strategy}")
        print()
        
        # ====================================================================
        # Step 1: ë¬¸ì„œ ë¶„ì„
        # ====================================================================
        print("\n" + "="*80)
        print("ğŸ“Š Step 1: ë¬¸ì„œ ë¶„ì„")
        print("="*80)
        
        analysis_result = self.analysis_node.analyze_documents(sources)
        
        if save_intermediate:
            analysis_path = os.path.join(self.output_dir, "json", "01_analysis.json")
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(analysis_result), f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_path}")
        
        # ====================================================================
        # Step 2: í† í”½ ì¶”ì¶œ
        # ====================================================================
        print("\n" + "="*80)
        print("ğŸ” Step 2: í† í”½ ì¶”ì¶œ")
        print("="*80)
        
        topics = self.topic_node.extract_topics_from_analysis(
            asdict(analysis_result),
            min_topics=min_topics,
            max_topics=max_topics
        )
        
        if not topics:
            print("âš ï¸  í† í”½ì´ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.")
            return {
                "analysis": analysis_result,
                "topics": [],
                "images": [],
                "paths": {}
            }
        
        # í† í”½ ìš”ì•½ ì¶œë ¥
        from app.nodes.topic_extraction_node import print_topics_summary
        print_topics_summary(topics)
        
        if save_intermediate:
            topics_path = os.path.join(self.output_dir, "json", "02_topics.json")
            with open(topics_path, 'w', encoding='utf-8') as f:
                topics_dict = [asdict(t) for t in topics]
                json.dump(topics_dict, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ í† í”½ ì €ì¥: {topics_path}")
        
        # ====================================================================
        # Step 3: ì´ë¯¸ì§€ ìƒì„±
        # ====================================================================
        print("\n" + "="*80)
        print("ğŸ¨ Step 3: ì´ë¯¸ì§€ ìƒì„±")
        print("="*80)
        
        images = self.image_node.generate_images_from_topics(
            topics,
            strategy=generation_strategy
        )
        
        if not images:
            print("âš ï¸  ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            paths = {
                "analysis_json": os.path.join(self.output_dir, "json", "01_analysis.json") if save_intermediate else None,
                "topics_json": os.path.join(self.output_dir, "json", "02_topics.json") if save_intermediate else None,
                "images_json": None,
                "gallery_html": None
            }
            
            return {
                "analysis": analysis_result,
                "topics": topics,
                "images": [],
                "paths": paths
            }
        
        # ì´ë¯¸ì§€ ìš”ì•½ ì¶œë ¥
        from app.nodes.image_generation_node import print_generation_summary
        print_generation_summary(images)
        
        if save_intermediate:
            images_path = os.path.join(self.output_dir, "json", "03_images.json")
            with open(images_path, 'w', encoding='utf-8') as f:
                images_dict = [asdict(img) for img in images]
                json.dump(images_dict, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ì´ë¯¸ì§€ ì •ë³´ ì €ì¥: {images_path}")
        
        # ====================================================================
        # Step 4: ê°¤ëŸ¬ë¦¬ ìƒì„±
        # ====================================================================
        print("\n" + "="*80)
        print("ğŸŒ Step 4: ê°¤ëŸ¬ë¦¬ ìƒì„±")
        print("="*80)
        
        from app.nodes.image_generation_node import create_image_gallery_html
        
        gallery_path = os.path.join(self.output_dir, "gallery.html")
        create_image_gallery_html(images, gallery_path)
        
        # ====================================================================
        # ìµœì¢… ê²°ê³¼
        # ====================================================================
        print("\n" + "="*80)
        print("âœ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*80)
        
        paths = {
            "analysis_json": os.path.join(self.output_dir, "json", "01_analysis.json") if save_intermediate else None,
            "topics_json": os.path.join(self.output_dir, "json", "02_topics.json") if save_intermediate else None,
            "images_json": os.path.join(self.output_dir, "json", "03_images.json") if save_intermediate else None,
            "gallery_html": gallery_path
        }
        
        print(f"\nğŸ“ ì¶œë ¥ í´ë”: {self.output_dir}")
        print(f"  - ì´ë¯¸ì§€: {os.path.join(self.output_dir, 'images')}/")
        print(f"  - JSON: {os.path.join(self.output_dir, 'json')}/")
        print(f"  - ê°¤ëŸ¬ë¦¬: {gallery_path}")
        
        print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"  - ë¶„ì„ëœ ë¬¸ì„œ: {len(sources)}ê°œ")
        print(f"  - ì¶”ì¶œëœ í† í”½: {len(topics)}ê°œ")
        print(f"  - ìƒì„±ëœ ì´ë¯¸ì§€: {len(images)}ê°œ")
        
        return {
            "analysis": analysis_result,
            "topics": topics,
            "images": images,
            "paths": paths
        }
    
    def run_from_texts(
        self,
        texts: List[str],
        **kwargs
    ) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì‹¤í–‰
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            **kwargs: run() ë©”ì„œë“œ ì¸ì
        
        Returns:
            run() ê²°ê³¼
        """
        sources = []
        for i, text in enumerate(texts):
            source = SourceDocument(
                id=f"doc_{i+1}",
                content=text,
                doc_type="text"
            )
            sources.append(source)
        
        return self.run(sources, **kwargs)


# ============================================================================
# í¸ì˜ í•¨ìˆ˜
# ============================================================================

def quick_pipeline(
    text: str,
    output_dir: str = "./quick_output",
    generation_strategy: str = "auto"
) -> Dict[str, Any]:
    """
    ë¹ ë¥¸ ë‹¨ì¼ í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        generation_strategy: ìƒì„± ì „ëµ
    
    Returns:
        íŒŒì´í”„ë¼ì¸ ê²°ê³¼
    """
    import vertexai
    
    # Vertex AI ì´ˆê¸°í™”
    vertexai.init(
        project="alan-document-lab",
        location="us-central1"
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = DocumentToImagePipeline(output_dir=output_dir)
    
    sources = [SourceDocument(id="quick_doc", content=text, doc_type="text")]
    
    return pipeline.run(sources, generation_strategy=generation_strategy)


def batch_pipeline(
    texts: List[str],
    output_dir: str = "./batch_output",
    generation_strategy: str = "auto"
) -> Dict[str, Any]:
    """
    ë°°ì¹˜ í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        generation_strategy: ìƒì„± ì „ëµ
    
    Returns:
        íŒŒì´í”„ë¼ì¸ ê²°ê³¼
    """
    import vertexai
    
    vertexai.init(
        project="alan-document-lab",
        location="us-central1"
    )
    
    pipeline = DocumentToImagePipeline(output_dir=output_dir)
    
    return pipeline.run_from_texts(texts, generation_strategy=generation_strategy)
