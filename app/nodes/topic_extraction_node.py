"""
í† í”½ ì¶”ì¶œ ë…¸ë“œ (LangGraph)
ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ìƒì„±ìš© í† í”½ ì¶”ì¶œ
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from vertexai.generative_models import GenerativeModel


@dataclass
class ImageTopic:
    """ì´ë¯¸ì§€ ìƒì„±ìš© í† í”½"""
    topic_id: str
    title: str
    description: str
    keywords: List[str]
    style: str  # abstract, technical, illustration, photo, scene
    importance: float  # 0.0 ~ 1.0
    context: Optional[str] = None  # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸


class TopicExtractionNode:
    """ìš”ì•½ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ìƒì„±ìš© í† í”½ ì¶”ì¶œ"""
    
    # í† í”½ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
    TOPIC_EXTRACTION_PROMPT = """ë‹¹ì‹ ì€ íŒŸìºìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì œì‘ì„ ìœ„í•œ ì´ë¯¸ì§€ í† í”½ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ê°€ ì£¼ì–´ì§€ë©´, **ë¹„ë””ì˜¤ ì˜¤ë²„ë ˆì´ìš© ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í† í”½**ì„ ì¶”ì¶œí•˜ì„¸ìš”.

[ì…ë ¥]
ë¬¸ì„œ ë¶„ì„ ìš”ì•½ ê²°ê³¼ (í†µí•© ìš”ì•½ ì„¹ì…˜ ì¤‘ì‹¬)

[ëª©í‘œ]
1. í†µí•© ìš”ì•½ì˜ ê° ì£¼ìš” ì„¹ì…˜/ê°œë…ì—ì„œ **ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” í† í”½** ì¶”ì¶œ
2. ê° í† í”½ì€ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ í‘œí˜„ ê°€ëŠ¥í•´ì•¼ í•¨
3. íŒŸìºìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì˜¤ë²„ë ˆì´ë¡œ ì‚¬ìš©ë  ê²ƒì„ ê³ ë ¤

[í† í”½ ì¶”ì¶œ ê¸°ì¤€]
âœ… í¬í•¨í•´ì•¼ í•  í† í”½:
- í•µì‹¬ ê°œë… (ì˜ˆ: "AI ê¸°ìˆ ", "ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤")
- ì£¼ìš” ì£¼ì œ ì „í™˜ì  (ì˜ˆ: "ì—­ì‚¬ì  ë°°ê²½", "í˜„ì¬ íŠ¸ë Œë“œ", "ë¯¸ë˜ ì „ë§")
- ì¤‘ìš”í•œ ë°ì´í„°/í†µê³„ (ì‹œê°í™” ê°€ëŠ¥í•œ)
- êµ¬ì²´ì ì¸ ì‚¬ë¡€/ì‘ìš© ë¶„ì•¼
- ë…¼ì˜ì˜ í•µì‹¬ í¬ì¸íŠ¸

âŒ ì œì™¸í•´ì•¼ í•  í† í”½:
- ë„ˆë¬´ ì¶”ìƒì ì´ê±°ë‚˜ ëª¨í˜¸í•œ ê°œë…
- ì‹œê°í™”ê°€ ë¶ˆê°€ëŠ¥í•œ ë‚´ìš©
- ì¤‘ë³µë˜ëŠ” ê°œë…

[í† í”½ ê°œìˆ˜]
- ìµœì†Œ: 5ê°œ
- ìµœëŒ€: 20ê°œ
- ê¶Œì¥: ë¶„ë‹¹ 1.5ê°œ (10ë¶„ ì½˜í…ì¸  = 15ê°œ í† í”½)
- ë¬¸ì„œ ê¸¸ì´ì™€ ë³µì¡ë„ì— ë”°ë¼ ì¡°ì ˆ

[ìŠ¤íƒ€ì¼ ì„ íƒ ê¸°ì¤€]
ê° í† í”½ì— ê°€ì¥ ì í•©í•œ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ì„¸ìš”:

1. **abstract** (ì¶”ìƒì  ë¯¸ë‹ˆë©€)
   - ê°œë…ì  ì£¼ì œ: "AIì˜ ë¯¸ë˜", "ë””ì§€í„¸ í˜ì‹ "
   - ë°°ê²½/ë¶„ìœ„ê¸° ì´ë¯¸ì§€
   - ì¥ì : ë¹ ë¥¸ ìƒì„±, ë¹„ìš© íš¨ìœ¨ì 

2. **technical** (ê¸°ìˆ  ë‹¤ì´ì–´ê·¸ë¨)
   - í”„ë¡œì„¸ìŠ¤/ì›Œí¬í”Œë¡œìš°: "ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸"
   - êµ¬ì¡°/ì•„í‚¤í…ì²˜: "ì‹ ê²½ë§ êµ¬ì¡°"
   - ì¥ì : ì •ë³´ ì „ë‹¬ë ¥, êµìœ¡ì 

3. **illustration** (ì°½ì˜ì  ì¼ëŸ¬ìŠ¤íŠ¸)
   - ê°œë… ì„¤ëª…: "ë”¥ëŸ¬ë‹ì˜ ì‘ë™ ì›ë¦¬"
   - ë¹„ìœ /ì€ìœ : "AIë¥¼ ì •ì› ê°€ê¾¸ê¸°ì— ë¹„ìœ "
   - ì¥ì : ì¹œê·¼í•¨, ì´í•´í•˜ê¸° ì‰¬ì›€

4. **photo** (í¬í† ë¦¬ì–¼ë¦¬ìŠ¤í‹±)
   - ì‹¤ì œ ì‚¬ë¡€: "ì˜ë£Œ í˜„ì¥ì˜ AI"
   - ì œí’ˆ/ì„œë¹„ìŠ¤: "AI ìŠ¤í”¼ì»¤"
   - ì¥ì : í˜„ì‹¤ê°, ê³µê°ëŒ€

5. **scene** (ì¥ë©´ ì¼ëŸ¬ìŠ¤íŠ¸)
   - ìŠ¤í† ë¦¬í…”ë§: "ì—°êµ¬ì›ì´ AI ëª¨ë¸ í›ˆë ¨í•˜ëŠ” ì¥ë©´"
   - ìƒí™© ë¬˜ì‚¬: "ë¯¸ë˜ ë„ì‹œì˜ ììœ¨ì£¼í–‰ì°¨"
   - ì¥ì : ëª°ì…ê°, ì„œì‚¬

[ì¶œë ¥ í˜•ì‹]
JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ê° í† í”½ì€ ë‹¤ìŒ êµ¬ì¡°:

```json
[
  {{
    "topic_id": "topic_01_opening",
    "title": "AI ê¸°ìˆ ì˜ ì‹œì‘",
    "description": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ì—­ì‚¬ì  ë°°ê²½ê³¼ ì´ˆê¸° ë°œì „ ê³¼ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ì¶”ìƒì  ì‹œê°í™”",
    "keywords": ["AI", "history", "technology", "innovation"],
    "style": "abstract",
    "importance": 0.9,
    "context": "ì˜¤í”„ë‹ ì¥ë©´, ì „ì²´ ë‚´ìš©ì˜ ë„ì…ë¶€"
  }},
  {{
    "topic_id": "topic_02_ml_process",
    "title": "ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤",
    "description": "ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬, í•™ìŠµ, í‰ê°€ ë‹¨ê³„ë¥¼ ë³´ì—¬ì£¼ëŠ” ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨",
    "keywords": ["machine learning", "process", "workflow", "data"],
    "style": "technical",
    "importance": 0.85,
    "context": "í•µì‹¬ ê°œë… ì„¤ëª… ì„¹ì…˜"
  }}
]
```

[ì¤‘ìš” ê·œì¹™]
1. **topic_idëŠ” ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë„˜ë²„ë§ í¬í•¨** (topic_01, topic_02...)
2. **descriptionì€ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ì˜ ê¸°ë°˜**ì´ ë¨ (êµ¬ì²´ì ìœ¼ë¡œ)
3. **keywordsëŠ” 5~10ê°œ** ì •ë„
4. **importanceëŠ” 0.0~1.0** (0.8 ì´ìƒì´ í•µì‹¬ í† í”½)
5. **styleì€ ë°˜ë“œì‹œ 5ê°€ì§€ ì¤‘ í•˜ë‚˜**: abstract, technical, illustration, photo, scene
6. **ì¤‘ë³µ ê°œë…ì€ í†µí•©**, í•˜ë‚˜ì˜ í† í”½ìœ¼ë¡œ

[ë¶„ì„í•  ìš”ì•½ ê²°ê³¼]
{summary_content}

ìœ„ ìš”ì•½ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì´ë¯¸ì§€ í† í”½ì„ ì¶”ì¶œí•˜ì„¸ìš”.
JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤."""

    def __init__(self, model_name: str = "gemini-2.5-flash"):
        """
        Args:
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸
        """
        self.model = GenerativeModel(model_name)
        self.model_name = model_name
    
    def extract_topics_from_analysis(
        self,
        analysis_result: Dict[str, Any],
        min_topics: int = 5,
        max_topics: int = 20,
        **generation_config
    ) -> List[ImageTopic]:
        """
        ë¶„ì„ ê²°ê³¼ì—ì„œ í† í”½ ì¶”ì¶œ
        
        Args:
            analysis_result: CompleteAnalysis (dict í˜•íƒœ)
            min_topics: ìµœì†Œ í† í”½ ê°œìˆ˜
            max_topics: ìµœëŒ€ í† í”½ ê°œìˆ˜
            **generation_config: Gemini ì„¤ì •
        
        Returns:
            ImageTopic ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ” í† í”½ ì¶”ì¶œ ì‹œì‘")
        
        # 1. í†µí•© ìš”ì•½ ì¶”ì¶œ
        integrated_summary = analysis_result.get('integrated_summary', {})
        
        # ì›ë³¸ ì¶œë ¥ì´ ìˆìœ¼ë©´ ì‚¬ìš© (ë” í’ë¶€í•œ ì •ë³´)
        raw_output = analysis_result.get('metadata', {}).get('raw_output', '')
        
        if raw_output:
            # ì›ë³¸ì—ì„œ í†µí•© ìš”ì•½ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ìˆë‹¤ë©´)
            summary_content = self._extract_summary_section(raw_output)
        else:
            # êµ¬ì¡°í™”ëœ ë°ì´í„° ì‚¬ìš©
            summary_content = self._format_integrated_summary(integrated_summary)
        
        print(f"ğŸ“ ìš”ì•½ ê¸¸ì´: {len(summary_content)} ë¬¸ì")
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.TOPIC_EXTRACTION_PROMPT.format(
            summary_content=summary_content
        )
        
        # 3. Gemini í˜¸ì¶œ
        print("ğŸ¤– Geminië¡œ í† í”½ ì¶”ì¶œ ì¤‘...")
        
        config = {
            "temperature": 0.4,  # ì ë‹¹íˆ ì°½ì˜ì 
            "top_p": 0.9,
            "max_output_tokens": 4096,
            **generation_config
        }
        
        response = self.model.generate_content(prompt, generation_config=config)
        raw_topics = response.text
        
        print(f"âœ… í† í”½ ì¶”ì¶œ ì™„ë£Œ")
        
        # 4. JSON íŒŒì‹±
        topics = self._parse_topics(raw_topics)
        
        # 5. ê°œìˆ˜ ì œí•œ
        if len(topics) < min_topics:
            print(f"âš ï¸  í† í”½ ê°œìˆ˜ ë¶€ì¡± ({len(topics)} < {min_topics})")
        elif len(topics) > max_topics:
            print(f"âš ï¸  í† í”½ ê°œìˆ˜ ì´ˆê³¼, ìƒìœ„ {max_topics}ê°œë§Œ ì‚¬ìš©")
            topics = sorted(topics, key=lambda t: t.importance, reverse=True)[:max_topics]
        
        print(f"ğŸ“Š ìµœì¢… í† í”½ ê°œìˆ˜: {len(topics)}")
        
        return topics
    
    def _extract_summary_section(self, raw_output: str) -> str:
        """ì›ë³¸ ì¶œë ¥ì—ì„œ í†µí•© ìš”ì•½ ì„¹ì…˜ ì¶”ì¶œ"""
        # "ìµœì¢… í†µí•© ìš”ì•½" ë˜ëŠ” "4. ğŸ“Œ" ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
        markers = [
            "4. ğŸ“Œ ìµœì¢… í†µí•© ìš”ì•½",
            "ìµœì¢… í†µí•© ìš”ì•½",
            "## ìµœì¢… í†µí•© ìš”ì•½",
            "### ìµœì¢… í†µí•© ìš”ì•½"
        ]
        
        for marker in markers:
            if marker in raw_output:
                parts = raw_output.split(marker, 1)
                if len(parts) > 1:
                    return marker + parts[1]
        
        # ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì „ì²´ ë°˜í™˜
        return raw_output
    
    def _format_integrated_summary(self, integrated_summary: Dict) -> str:
        """êµ¬ì¡°í™”ëœ í†µí•© ìš”ì•½ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        sections = integrated_summary.get('sections', [])
        conclusion = integrated_summary.get('conclusion', '')
        
        text_parts = []
        
        for section in sections:
            title = section.get('title', '')
            content = section.get('content', '')
            key_points = section.get('key_points', [])
            
            text_parts.append(f"## {title}")
            text_parts.append(content)
            if key_points:
                text_parts.append("í•µì‹¬ í¬ì¸íŠ¸:")
                for point in key_points:
                    text_parts.append(f"- {point}")
            text_parts.append("")
        
        if conclusion:
            text_parts.append("## ê²°ë¡ ")
            text_parts.append(conclusion)
        
        return "\n".join(text_parts)
    
    def _parse_topics(self, raw_topics: str) -> List[ImageTopic]:
        """JSON ë¬¸ìì—´ì„ ImageTopic ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±"""
        import json
        import re
        
        # JSON ì¶”ì¶œ (markdown ì½”ë“œ ë¸”ë¡ ì œê±°)
        json_text = raw_topics
        
        # ```json ... ``` ì œê±°
        json_text = re.sub(r'```json\s*', '', json_text)
        json_text = re.sub(r'```\s*', '', json_text)
        json_text = json_text.strip()
        
        try:
            topics_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {json_text[:200]}...")
            return []
        
        # ImageTopic ê°ì²´ ìƒì„±
        topics = []
        for data in topics_data:
            try:
                topic = ImageTopic(
                    topic_id=data.get('topic_id', f"topic_{len(topics)+1:02d}"),
                    title=data.get('title', ''),
                    description=data.get('description', ''),
                    keywords=data.get('keywords', []),
                    style=data.get('style', 'abstract'),
                    importance=float(data.get('importance', 0.5)),
                    context=data.get('context')
                )
                topics.append(topic)
            except Exception as e:
                print(f"âš ï¸  í† í”½ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        return topics
    
    def __call__(self, state: dict) -> dict:
        """
        LangGraph ë…¸ë“œ ì‹¤í–‰
        
        Expected state:
            - analysis_result: CompleteAnalysis (dict)
        
        Returns:
            - image_topics: List[ImageTopic]
        """
        analysis_result = state.get("analysis_result")
        
        if not analysis_result:
            raise ValueError("No analysis_result in state")
        
        # CompleteAnalysisê°€ dataclassë©´ dictë¡œ ë³€í™˜
        if hasattr(analysis_result, '__dict__'):
            from dataclasses import asdict
            analysis_result = asdict(analysis_result)
        
        topics = self.extract_topics_from_analysis(analysis_result)
        
        return {
            **state,
            "image_topics": topics
        }


# ============================================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================================

def print_topics_summary(topics: List[ImageTopic]):
    """í† í”½ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ì¶”ì¶œëœ ì´ë¯¸ì§€ í† í”½")
    print("="*80)
    
    print(f"\nì´ {len(topics)}ê°œ í† í”½")
    
    # ìŠ¤íƒ€ì¼ë³„ ë¶„í¬
    style_counts = {}
    for topic in topics:
        style_counts[topic.style] = style_counts.get(topic.style, 0) + 1
    
    print("\nìŠ¤íƒ€ì¼ ë¶„í¬:")
    for style, count in sorted(style_counts.items()):
        print(f"  {style}: {count}ê°œ")
    
    # í† í”½ ëª©ë¡
    print("\n" + "-"*80)
    print("í† í”½ ìƒì„¸")
    print("-"*80)
    
    for i, topic in enumerate(topics, 1):
        print(f"\n[{i}] {topic.topic_id}")
        print(f"  ì œëª©: {topic.title}")
        print(f"  ìŠ¤íƒ€ì¼: {topic.style}")
        print(f"  ì¤‘ìš”ë„: {topic.importance:.2f}")
        print(f"  ì„¤ëª…: {topic.description[:100]}...")
        print(f"  í‚¤ì›Œë“œ: {', '.join(topic.keywords[:5])}")
        if topic.context:
            print(f"  ì»¨í…ìŠ¤íŠ¸: {topic.context}")


def save_topics_to_json(topics: List[ImageTopic], output_path: str):
    """í† í”½ì„ JSONìœ¼ë¡œ ì €ì¥"""
    import json
    from dataclasses import asdict
    
    topics_dict = [asdict(topic) for topic in topics]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(topics_dict, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ í† í”½ ì €ì¥: {output_path}")
