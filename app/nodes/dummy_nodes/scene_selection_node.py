"""
ì¥ë©´ ì„ íƒ ë…¸ë“œ (LangGraph)
ì ì‘í˜• í”„ë¡œë“€ì„œ í˜ë¥´ì†Œë‚˜ë¡œ ì´ë¯¸ì§€ í•„ìš” ì¥ë©´ ìë™ ì„ íƒ
"""

import json
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# Vertex AI import (ì„¤ì¹˜ í•„ìš”: pip install google-cloud-aiplatform)
try:
    import vertexai
    from vertexai.generative_models import GenerativeModel
    VERTEXAI_AVAILABLE = True
except ImportError:
    VERTEXAI_AVAILABLE = False
    print("âš ï¸  vertexai íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ íŒë‹¨ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")


# PodcastScene import
try:
    from script_parser_node import PodcastScene
except ImportError:
    # ìƒëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„
    import sys
    import os
    sys.path.insert(0, os.path.dirname(__file__))
    try:
        from script_parser_node import PodcastScene
    except ImportError:
        print("âš ï¸  script_parser_nodeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        PodcastScene = None


class SceneSelectionNode:
    """
    ì¥ë©´ ì„ íƒ ë…¸ë“œ - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ íŒë‹¨
    
    íŠ¹ì§•:
    - ë©”íƒ€ë°ì´í„° í™œìš© (ì±•í„°, í•µì‹¬ ê°œë…, ì„ê³„ ìˆœê°„)
    - ì±•í„°ë³„ ì´ë¯¸ì§€ ë°°ë¶„
    - í•µì‹¬ ê°œë… ìš°ì„  ì‹œê°í™”
    - ì¤‘ë³µ ë°©ì§€
    """
    
    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
    METADATA_BASED_PROMPT = """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ì¥ë¥´ì˜ íŒŸìºìŠ¤íŠ¸ ë¹„ë””ì˜¤ë¥¼ ì œì‘í•˜ëŠ” ë² í…Œë‘ í”„ë¡œë“€ì„œì…ë‹ˆë‹¤.

**ë‹¹ì‹ ì˜ ê²½í—˜:**
- êµìœ¡ ì½˜í…ì¸ : Khan Academy, Kurzgesagt, Crash Course
- ë‰´ìŠ¤/ì‹œì‚¬: CNN, BBC
- ìŠ¤í† ë¦¬í…”ë§: Netflix ë‹¤íë©˜í„°ë¦¬
- ë¹„ì¦ˆë‹ˆìŠ¤: TED, ê¸°ì—… IR
- ì¸í„°ë·°/ëŒ€ë‹´: Joe Rogan, Lex Fridman
- ì—”í„°í…Œì¸ë¨¼íŠ¸: YouTube ë²„ë¼ì´ì–´í‹°

**ì „ì²´ ì»¨í…ìŠ¤íŠ¸:**
- ì½˜í…ì¸  íƒ€ì…: {content_type}
- ì „ì²´ ìš”ì•½: {summary}
- ì „ì²´ ë¬´ë“œ: {overall_mood}

**í˜„ì¬ ì±•í„°:**
- ì œëª©: {chapter_title}
- ì¤‘ìš”ë„: {chapter_importance}
- ì£¼ìš” ì£¼ì œ: {chapter_topics}
- ì´ ì±•í„° ì˜ˆìƒ ì´ë¯¸ì§€: {chapter_expected_images}ê°œ
- ì´ë¯¸ ì„ íƒëœ ì´ë¯¸ì§€: {chapter_selected_count}ê°œ

**í•µì‹¬ ê°œë… (ì‹œê°í™” ëŒ€ìƒ):**
{key_concepts}

**í˜„ì¬ ì¥ë©´:**
ì‹œê°„: {timestamp_start} ({duration}ì´ˆ)
í™”ì: {speaker}
ë‚´ìš©: "{text}"

**ì• ì¥ë©´:**
{prev_text}

**ë‹¤ìŒ ì¥ë©´:**
{next_text}

**íŒë‹¨ ê¸°ì¤€:**

1. **ì±•í„° ë°°ë¶„ ì²´í¬:**
   - ì´ ì±•í„°ì—ì„œ ì´ë¯¸ {chapter_selected_count}ê°œ ì„ íƒë¨
   - ëª©í‘œ: {chapter_expected_images}ê°œ
   - ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜

2. **í•µì‹¬ ê°œë… ìš°ì„ :**
   - í•µì‹¬ ê°œë… ì²« ë“±ì¥ â†’ ë°˜ë“œì‹œ ì‹œê°í™”
   - ê°™ì€ ê°œë… ë°˜ë³µ â†’ ì²« ë“±ì¥ë§Œ

3. **ì¤‘ë³µ ë°©ì§€:**
   - ì• ì¥ë©´ê³¼ ë¹„ìŠ·í•œ ë‚´ìš©? â†’ ìŠ¤í‚µ
   - ì´ë¯¸ ê°™ì€ ì£¼ì œ ì´ë¯¸ì§€ ìˆìŒ? â†’ ìŠ¤í‚©

4. **ì„ê³„ ìˆœê°„:**
   - Critical Moment ì¥ë©´ â†’ ìš°ì„  ì„ íƒ

5. **ë‚´ìš© ê°€ì¹˜:**
   - êµ¬ì²´ì  ì„¤ëª…/ì˜ˆì‹œ â†’ ì„ íƒ
   - ë‹¨ìˆœ ì§ˆë¬¸/ë°˜ì‘ â†’ ìŠ¤í‚µ

**ì´ë¯¸ì§€ê°€ í•„ìš”í•œê°€?**

JSON ì‘ë‹µ:
{{
    "image_required": true/false,
    "importance": 0.0-1.0,
    "content_nature": "{content_type}",
    "visual_type": "concept/technical/example/persona/scene/none",
    "reason": "í•œ ë¬¸ì¥ ì„¤ëª…"
}}
"""
    
    def __init__(
        self,
        project_id: str = None,
        location: str = "us-central1",
        model_name: str = "gemini-2.5-flash"
    ):
        """ì¥ë©´ ì„ íƒ ë…¸ë“œ ì´ˆê¸°í™”"""
        # í”„ë¡œì íŠ¸ ID ì²˜ë¦¬
        if project_id is None:
            import os
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT") or os.getenv("GCP_PROJECT")
            if not project_id:
                print("âš ï¸  í”„ë¡œì íŠ¸ ID ì—†ìŒ - ë”ë¯¸ íŒë‹¨ ì‚¬ìš©")
        
        self.project_id = project_id
        self.location = location
        self.model_name = model_name
        
        # Vertex AI ì´ˆê¸°í™”
        if VERTEXAI_AVAILABLE and project_id:
            try:
                vertexai.init(project=project_id, location=location)
                self.model = GenerativeModel(model_name)
                print(f"âœ… Vertex AI ì´ˆê¸°í™” ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âš ï¸  Vertex AI ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                self.model = None
        else:
            self.model = None
    
    def judge_scene_with_metadata(
        self,
        scene: PodcastScene,
        metadata: Any,  # PodcastMetadata
        prev_scene: Optional[PodcastScene] = None,
        next_scene: Optional[PodcastScene] = None,
        chapter_selected_count: int = 0
    ) -> Dict[str, Any]:
        """
        ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¥ë©´ íŒë‹¨
        """
        if not self.model:
            # ë”ë¯¸ íŒë‹¨
            return self._dummy_judgment(scene, metadata, chapter_selected_count)
        
        # í˜„ì¬ ì¥ë©´ì˜ ì±•í„° ì°¾ê¸°
        chapter = self._find_chapter(scene, metadata.content.chapters)
        
        # í•µì‹¬ ê°œë… ëª©ë¡
        key_concepts_text = "\n".join([
            f"  - {kc.term} (ìš°ì„ ìˆœìœ„: {kc.visual_priority}, ì²« ë“±ì¥: {kc.first_appearance})"
            for kc in metadata.content.key_concepts
            if kc.should_visualize
        ])
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.METADATA_BASED_PROMPT.format(
            content_type=metadata.content.content_type,
            summary=metadata.content.summary[:200] + "...",
            overall_mood=metadata.visual.overall_mood,
            chapter_title=chapter.title if chapter else "ì•Œ ìˆ˜ ì—†ìŒ",
            chapter_importance=chapter.importance if chapter else 0.5,
            chapter_topics=", ".join(chapter.key_topics) if chapter else "",
            chapter_expected_images=chapter.expected_images if chapter else 1,
            chapter_selected_count=chapter_selected_count,
            key_concepts=key_concepts_text,
            timestamp_start=scene.timestamp_start,
            duration=scene.duration,
            speaker=scene.speaker,
            text=scene.text,
            prev_text=prev_scene.text[:100] if prev_scene else "ì—†ìŒ",
            next_text=next_scene.text[:100] if next_scene else "ì—†ìŒ"
        )
        
        try:
            # Gemini í˜¸ì¶œ
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "max_output_tokens": 500
                }
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.text.strip()
            response_text = re.sub(r'```json\s*', '', response_text)
            response_text = re.sub(r'```\s*', '', response_text)
            
            judgment = json.loads(response_text)
            return judgment
        
        except Exception as e:
            print(f"âš ï¸  íŒë‹¨ ì‹¤íŒ¨: {str(e)}")
            return self._dummy_judgment(scene, metadata, chapter_selected_count)
    
    def _find_chapter(self, scene: PodcastScene, chapters: List) -> Any:
        """ì¥ë©´ì´ ì†í•œ ì±•í„° ì°¾ê¸°"""
        scene_time = self._time_to_seconds(scene.timestamp_start)
        
        for chapter in chapters:
            start = self._time_to_seconds(chapter.start_time)
            end = self._time_to_seconds(chapter.end_time)
            
            if start <= scene_time < end:
                return chapter
        
        return None
    
    def _time_to_seconds(self, time_str: str) -> int:
        """ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë¡œ ë³€í™˜"""
        parts = time_str.split(":")
        if len(parts) == 3:
            h, m, s = map(int, parts)
            return h * 3600 + m * 60 + s
        return 0
    
    def _dummy_judgment(
        self,
        scene: PodcastScene,
        metadata: Any,
        chapter_selected_count: int
    ) -> Dict[str, Any]:
        """ë”ë¯¸ íŒë‹¨ - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê·œì¹™"""
        
        # ì±•í„° ì°¾ê¸°
        chapter = self._find_chapter(scene, metadata.content.chapters)
        
        # ì´ë¯¸ ì±•í„° ëª©í‘œ ë‹¬ì„±?
        if chapter and chapter_selected_count >= chapter.expected_images:
            return {
                "image_required": False,
                "importance": 0.3,
                "content_nature": metadata.content.content_type,
                "visual_type": "none",
                "reason": "ì±•í„° ì´ë¯¸ì§€ ëª©í‘œ ë‹¬ì„±"
            }
        
        # í•µì‹¬ ê°œë… ë“±ì¥?
        for concept in metadata.content.key_concepts:
            if concept.term in scene.text and concept.should_visualize:
                return {
                    "image_required": True,
                    "importance": 0.9,
                    "content_nature": metadata.content.content_type,
                    "visual_type": "concept",
                    "reason": f"í•µì‹¬ ê°œë… '{concept.term}' ë“±ì¥"
                }
        
        # ì§§ì€ ì¥ë©´?
        if scene.duration < 10:
            return {
                "image_required": False,
                "importance": 0.2,
                "content_nature": metadata.content.content_type,
                "visual_type": "none",
                "reason": "ì§§ì€ ì¥ë©´"
            }
        
        # ê¸°ë³¸ íŒë‹¨
        return {
            "image_required": False,
            "importance": 0.5,
            "content_nature": metadata.content.content_type,
            "visual_type": "none",
            "reason": "ì¼ë°˜ ì¥ë©´"
        }
    
    def select_scenes_with_metadata(
        self,
        scenes: List[PodcastScene],
        metadata: Any,  # PodcastMetadata
        show_progress: bool = True
    ) -> List[PodcastScene]:
        """
        ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¥ë©´ ì„ íƒ
        """
        print(f"\nğŸ¬ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¥ë©´ ì„ íƒ ì‹œì‘")
        print("="*80)
        print(f"ì´ ì¥ë©´: {len(scenes)}ê°œ")
        print(f"ì½˜í…ì¸  íƒ€ì…: {metadata.content.content_type}")
        print(f"ì±•í„°: {len(metadata.content.chapters)}ê°œ")
        
        selected = []
        chapter_counts = {}  # ì±•í„°ë³„ ì„ íƒ ì¹´ìš´íŠ¸
        
        for i, scene in enumerate(scenes):
            if show_progress:
                print(f"\n[{i+1}/{len(scenes)}] {scene.scene_id} ë¶„ì„ ì¤‘...")
                print(f"  ë‚´ìš©: {scene.text[:60]}...")
            
            # ì•ë’¤ ì¥ë©´
            prev_scene = scenes[i-1] if i > 0 else None
            next_scene = scenes[i+1] if i < len(scenes)-1 else None
            
            # í˜„ì¬ ì±•í„° ì°¾ê¸°
            chapter = self._find_chapter(scene, metadata.content.chapters)
            chapter_id = chapter.id if chapter else "unknown"
            
            # ì±•í„°ë³„ ì¹´ìš´íŠ¸ í™•ì¸
            chapter_selected = chapter_counts.get(chapter_id, 0)
            
            # íŒë‹¨
            judgment = self.judge_scene_with_metadata(
                scene=scene,
                metadata=metadata,
                prev_scene=prev_scene,
                next_scene=next_scene,
                chapter_selected_count=chapter_selected
            )
            
            # ê²°ê³¼ ì ìš©
            scene.image_required = judgment.get("image_required", False)
            scene.importance = judgment.get("importance", 0.5)
            scene.context = judgment.get("reason", "")
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            if not hasattr(scene, 'content_nature'):
                scene.__dict__['content_nature'] = judgment.get("content_nature", "unknown")
            if not hasattr(scene, 'visual_type'):
                scene.__dict__['visual_type'] = judgment.get("visual_type", "none")
            if not hasattr(scene, 'chapter_id'):
                scene.__dict__['chapter_id'] = chapter_id
            
            if show_progress:
                status = "âœ… ì´ë¯¸ì§€ í•„ìš”" if scene.image_required else "âŒ ì´ë¯¸ì§€ ë¶ˆí•„ìš”"
                print(f"  {status} (ì¤‘ìš”ë„: {scene.importance:.2f})")
                print(f"  ì±•í„°: {chapter.title if chapter else 'ì•Œ ìˆ˜ ì—†ìŒ'}")
                print(f"  ì´ìœ : {scene.context}")
            
            # ì„ íƒëœ ì¥ë©´
            if scene.image_required:
                selected.append(scene)
                chapter_counts[chapter_id] = chapter_selected + 1
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary_with_metadata(scenes, selected, metadata, chapter_counts)
        
        return selected
    
    def _print_summary_with_metadata(
        self,
        all_scenes: List,
        selected_scenes: List,
        metadata: Any,
        chapter_counts: Dict
    ):
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¯ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì„ íƒ ì™„ë£Œ")
        print("="*80)
        
        total_duration = sum(s.duration for s in all_scenes)
        avg_interval = total_duration / len(selected_scenes) if selected_scenes else 0
        
        print(f"\nğŸ“Š í†µê³„:")
        print(f"  ì´ ì¥ë©´: {len(all_scenes)}ê°œ")
        print(f"  ì´ë¯¸ì§€ ìƒì„±: {len(selected_scenes)}ê°œ")
        print(f"  í‰ê·  ê°„ê²©: {avg_interval:.1f}ì´ˆ")
        
        # ì±•í„°ë³„ ë¶„ì„
        print(f"\nğŸ“š ì±•í„°ë³„ ì´ë¯¸ì§€ ë°°ë¶„:")
        for chapter in metadata.content.chapters:
            expected = chapter.expected_images
            actual = chapter_counts.get(chapter.id, 0)
            status = "âœ…" if actual <= expected else "âš ï¸"
            print(f"  {status} {chapter.title}: {actual}/{expected}ê°œ (ì¤‘ìš”ë„: {chapter.importance:.2f})")
        
        # ì¤‘ìš”ë„ ë¶„í¬
        if selected_scenes:
            high = len([s for s in selected_scenes if s.importance >= 0.8])
            medium = len([s for s in selected_scenes if 0.5 <= s.importance < 0.8])
            low = len([s for s in selected_scenes if s.importance < 0.5])
            
            print(f"\nâ­ ì¤‘ìš”ë„ ë¶„í¬:")
            print(f"  ë†’ìŒ (â‰¥0.8): {high}ê°œ")
            print(f"  ì¤‘ê°„ (0.5-0.8): {medium}ê°œ")
            print(f"  ë‚®ìŒ (<0.5): {low}ê°œ")
        
        print("="*80)
    
    # ì ì‘í˜• í”„ë¡œë“€ì„œ í”„ë¡¬í”„íŠ¸
    ADAPTIVE_PRODUCER_PROMPT = """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ì¥ë¥´ì˜ íŒŸìºìŠ¤íŠ¸ ë¹„ë””ì˜¤ë¥¼ ì œì‘í•˜ëŠ” ë² í…Œë‘ í”„ë¡œë“€ì„œì…ë‹ˆë‹¤.

**ë‹¹ì‹ ì˜ ê²½í—˜:**
- êµìœ¡ ì½˜í…ì¸ : Khan Academy, Kurzgesagt, Crash Course ìŠ¤íƒ€ì¼
- ë‰´ìŠ¤/ì‹œì‚¬: CNN, BBC ë‰´ìŠ¤ ë¹„ë””ì˜¤
- ìŠ¤í† ë¦¬í…”ë§: Netflix ë‹¤íë©˜í„°ë¦¬, ì˜¤ë””ì˜¤ë¶ ë¹„ë””ì˜¤
- ë¹„ì¦ˆë‹ˆìŠ¤: TED ê°•ì—°, ê¸°ì—… IR í”„ë ˆì  í…Œì´ì…˜
- ì¸í„°ë·°/ëŒ€ë‹´: Joe Rogan, Lex Fridman íŒŸìºìŠ¤íŠ¸
- ì—”í„°í…Œì¸ë¨¼íŠ¸: YouTube ë²„ë¼ì´ì–´í‹°

**ë‹¹ì‹ ì˜ ì„±ê³¼:**
- ì´ ì¡°íšŒìˆ˜ 1ì–µ+ ë‹¬ì„±
- ì‹œì²­ ì™„ë£Œìœ¨ í‰ê·  85%
- NotebookLM ìŠ¤íƒ€ì¼ ë¹„ë””ì˜¤ ì œì‘ ì „ë¬¸

**íŒë‹¨ ì›ì¹™:**

ğŸ¯ **í•µì‹¬ ì§ˆë¬¸: "ì´ ì¥ë©´ì— ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì‹œì²­ì ê²½í—˜ì´ ë” ì¢‹ì•„ì§ˆê¹Œ?"**

ğŸ“š **êµìœ¡/ì„¤ëª… ì½˜í…ì¸ :**
- ì¶”ìƒì  ê°œë…, ê¸°ìˆ  â†’ ë°˜ë“œì‹œ ì‹œê°í™” (ì˜ˆ: AI, ë¨¸ì‹ ëŸ¬ë‹, ë¸”ë¡ì²´ì¸)
- í”„ë¡œì„¸ìŠ¤, ì›Œí¬í”Œë¡œìš° â†’ ë°˜ë“œì‹œ ì‹œê°í™” (ì˜ˆ: ì‘ë™ ì›ë¦¬, ë‹¨ê³„)
- ì˜ˆì‹œ, ì‚¬ë¡€ â†’ ì‹œê°í™” ê¶Œì¥

ğŸ“° **ë‰´ìŠ¤/ì‹œì‚¬:**
- ë°ì´í„°, í†µê³„, ìˆ˜ì¹˜ â†’ ë°˜ë“œì‹œ ì‹œê°í™”
- ì¸ë¬¼, ì¥ì†Œ, ì‚¬ê±´ â†’ ì‹œê°í™” ê¶Œì¥
- ë‹¨ìˆœ ì˜ê²¬ â†’ ì‹œê°í™” ë¶ˆí•„ìš”

ğŸ“– **ìŠ¤í† ë¦¬í…”ë§:**
- ì£¼ìš” ì¥ë©´, ì „í™˜ì  â†’ ì‹œê°í™” ê¶Œì¥
- ë¶„ìœ„ê¸° ì „í™˜ â†’ ì‹œê°í™” ì„ íƒì 
- ë‹¨ìˆœ ë¬˜ì‚¬ â†’ ì‹œê°í™” ë¶ˆí•„ìš”

ğŸ’¼ **ë¹„ì¦ˆë‹ˆìŠ¤:**
- ìˆ«ì, ê·¸ë˜í”„, ì„±ê³¼ â†’ ë°˜ë“œì‹œ ì‹œê°í™”
- ì „ëµ, êµ¬ì¡°, ì¡°ì§ë„ â†’ ë°˜ë“œì‹œ ì‹œê°í™”
- ì¼ë°˜ë¡  â†’ ì‹œê°í™” ë¶ˆí•„ìš”

ğŸ¤ **ì¸í„°ë·°/ëŒ€ë‹´:**
- ì£¼ì œ ì „í™˜, í•µì‹¬ í¬ì¸íŠ¸ â†’ ì‹œê°í™” ì„ íƒì 
- ë‹¨ìˆœ ëŒ€í™”, ë°˜ì‘ â†’ ì‹œê°í™” ë¶ˆí•„ìš”
- êµ¬ì²´ì  ì‚¬ë¡€ ì–¸ê¸‰ â†’ ì‹œê°í™” ê¶Œì¥

âŒ **ì ˆëŒ€ ì´ë¯¸ì§€ ë¶ˆí•„ìš”:**
- ì¸ì‚¬, ë§ˆë¬´ë¦¬ ("ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤")
- ì§§ì€ ì§ˆë¬¸ (10ì´ˆ ë¯¸ë§Œ)
- ë‹¨ìˆœ ë°˜ì‘, ì¶”ì„ìƒˆ ("ì•„", "ì™€", "ê·¸ë ‡êµ°ìš”", "ë„¤")
- ì—°ê²° ë©˜íŠ¸
- ì´ë¯¸ì§€ê°€ ì˜¤íˆë ¤ ë°©í•´ê°€ ë˜ëŠ” ê²½ìš°

---

**ì´ì œ ë‹¤ìŒ ì¥ë©´ì„ ë¶„ì„í•˜ì„¸ìš”:**

ì‹œê°„: {timestamp_start} (ê¸¸ì´: {duration}ì´ˆ)
í™”ì: {speaker}
ë‚´ìš©: "{text}"

**ë¶„ì„ ë‹¨ê³„:**
1. ì´ ì¥ë©´ì˜ ì½˜í…ì¸  ì„±ê²© íŒŒì•… (êµìœ¡/ë‰´ìŠ¤/ìŠ¤í† ë¦¬/ë¹„ì¦ˆë‹ˆìŠ¤/ì¸í„°ë·°)
2. ì‹œê°í™” ê°€ì¹˜ íŒë‹¨ (ë„ì›€ ë˜ëŠ”ê°€? ë¶ˆí•„ìš”í•œê°€?)
3. ì¤‘ìš”ë„ í‰ê°€ (0.0-1.0)

**ì „ë¬¸ í”„ë¡œë“€ì„œë¡œì„œ íŒë‹¨í•˜ì„¸ìš”.**

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
{{
    "image_required": true ë˜ëŠ” false,
    "importance": 0.0ì—ì„œ 1.0 ì‚¬ì´ ìˆ«ì,
    "content_nature": "educational/news/story/business/interview/entertainment ì¤‘ í•˜ë‚˜",
    "visual_type": "concept/technical/data/scene/person/diagram/atmosphere/none ì¤‘ í•˜ë‚˜",
    "reason": "í”„ë¡œë“€ì„œ ê´€ì ì—ì„œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…"
}}"""
    
    def __init__(
        self,
        project_id: str = "alan-document-lab",
        location: str = "us-central1",
        model_name: str = "gemini-2.5-flash"
    ):
        """
        ì¥ë©´ ì„ íƒ ë…¸ë“œ ì´ˆê¸°í™”
        
        Args:
            project_id: Google Cloud í”„ë¡œì íŠ¸ ID
            location: Vertex AI ë¦¬ì „
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸
        """
        self.project_id = project_id
        self.location = location
        self.model_name = model_name
        
        # Vertex AI ì´ˆê¸°í™”
        if VERTEXAI_AVAILABLE:
            try:
                vertexai.init(project=project_id, location=location)
                self.model = GenerativeModel(model_name)
                print(f"âœ… Vertex AI ì´ˆê¸°í™” ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âš ï¸  Vertex AI ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                self.model = None
        else:
            self.model = None
    
    def judge_scene(self, scene: PodcastScene) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì¥ë©´ íŒë‹¨ - ì ì‘í˜• í”„ë¡œë“€ì„œ ì‹œê°
        
        Args:
            scene: PodcastScene ê°ì²´
        
        Returns:
            íŒë‹¨ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.model:
            # Vertex AI ì—†ìœ¼ë©´ ë”ë¯¸ ì‘ë‹µ
            return {
                "image_required": False,
                "importance": 0.5,
                "content_nature": "unknown",
                "visual_type": "none",
                "reason": "Vertex AI ë¯¸ì‚¬ìš©"
            }
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.ADAPTIVE_PRODUCER_PROMPT.format(
            timestamp_start=scene.timestamp_start,
            duration=scene.duration,
            speaker=scene.speaker,
            text=scene.text
        )
        
        try:
            # Gemini í˜¸ì¶œ
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ê²Œ
                    "top_p": 0.8,
                    "max_output_tokens": 500
                }
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.text.strip()
            
            # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°)
            response_text = re.sub(r'```json\s*', '', response_text)
            response_text = re.sub(r'```\s*', '', response_text)
            response_text = response_text.strip()
            
            # JSON íŒŒì‹±
            judgment = json.loads(response_text)
            
            return judgment
        
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"ì›ë³¸ ì‘ë‹µ: {response_text[:200]}")
            
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "image_required": False,
                "importance": 0.5,
                "content_nature": "unknown",
                "visual_type": "none",
                "reason": "íŒŒì‹± ì‹¤íŒ¨"
            }
        
        except Exception as e:
            print(f"âš ï¸  ì¥ë©´ íŒë‹¨ ì‹¤íŒ¨: {str(e)}")
            return {
                "image_required": False,
                "importance": 0.5,
                "content_nature": "unknown",
                "visual_type": "none",
                "reason": f"ì˜¤ë¥˜: {str(e)}"
            }
    
    def select_scenes(
        self,
        scenes: List[PodcastScene],
        show_progress: bool = True
    ) -> List[PodcastScene]:
        """
        ëª¨ë“  ì¥ë©´ ìˆœíšŒí•˜ë©° ë…ë¦½ íŒë‹¨
        
        Args:
            scenes: PodcastScene ë¦¬ìŠ¤íŠ¸
            show_progress: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            ì´ë¯¸ì§€ í•„ìš”í•œ ì¥ë©´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ¬ ì¥ë©´ ì„ íƒ ì‹œì‘ (ì´ {len(scenes)}ê°œ)")
        print("="*80)
        
        selected = []
        
        for i, scene in enumerate(scenes):
            if show_progress:
                print(f"\n[{i+1}/{len(scenes)}] {scene.scene_id} ë¶„ì„ ì¤‘...")
                print(f"  ë‚´ìš©: {scene.text[:60]}...")
            
            # íŒë‹¨
            judgment = self.judge_scene(scene)
            
            # ê²°ê³¼ ì ìš©
            scene.image_required = judgment.get("image_required", False)
            scene.importance = judgment.get("importance", 0.5)
            scene.context = judgment.get("reason", "")
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            if not hasattr(scene, 'content_nature'):
                scene.__dict__['content_nature'] = judgment.get("content_nature", "unknown")
            if not hasattr(scene, 'visual_type'):
                scene.__dict__['visual_type'] = judgment.get("visual_type", "none")
            
            if show_progress:
                status = "âœ… ì´ë¯¸ì§€ í•„ìš”" if scene.image_required else "âŒ ì´ë¯¸ì§€ ë¶ˆí•„ìš”"
                print(f"  {status} (ì¤‘ìš”ë„: {scene.importance:.2f})")
                print(f"  ì´ìœ : {scene.context}")
            
            # ì„ íƒëœ ì¥ë©´ë§Œ ì¶”ê°€
            if scene.image_required:
                selected.append(scene)
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary(scenes, selected)
        
        return selected
    
    def _print_summary(self, all_scenes: List, selected_scenes: List):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¯ ì¥ë©´ ì„ íƒ ì™„ë£Œ")
        print("="*80)
        
        total_duration = sum(s.duration for s in all_scenes)
        avg_interval = total_duration / len(selected_scenes) if selected_scenes else 0
        
        print(f"\nğŸ“Š í†µê³„:")
        print(f"  ì´ ì¥ë©´: {len(all_scenes)}ê°œ")
        print(f"  ì´ë¯¸ì§€ ìƒì„±: {len(selected_scenes)}ê°œ")
        print(f"  ì´ë¯¸ì§€ ë¹„ìœ¨: {len(selected_scenes)/len(all_scenes)*100:.1f}%")
        print(f"  í‰ê·  ê°„ê²©: {avg_interval:.1f}ì´ˆ")
        
        # ì½˜í…ì¸  íƒ€ì… ë¶„í¬
        if selected_scenes:
            content_types = {}
            for scene in selected_scenes:
                ctype = getattr(scene, 'content_nature', 'unknown')
                content_types[ctype] = content_types.get(ctype, 0) + 1
            
            print(f"\nğŸ“š ì½˜í…ì¸  íƒ€ì… ë¶„í¬:")
            for ctype, count in content_types.items():
                print(f"  {ctype}: {count}ê°œ")
        
        # ì¤‘ìš”ë„ ë¶„í¬
        if selected_scenes:
            high = len([s for s in selected_scenes if s.importance >= 0.8])
            medium = len([s for s in selected_scenes if 0.5 <= s.importance < 0.8])
            low = len([s for s in selected_scenes if s.importance < 0.5])
            
            print(f"\nâ­ ì¤‘ìš”ë„ ë¶„í¬:")
            print(f"  ë†’ìŒ (â‰¥0.8): {high}ê°œ")
            print(f"  ì¤‘ê°„ (0.5-0.8): {medium}ê°œ")
            print(f"  ë‚®ìŒ (<0.5): {low}ê°œ")
        
        # ì•ˆì „ ê²½ê³ 
        self._safety_check(all_scenes, selected_scenes)
        
        print("="*80)
    
    def _safety_check(self, all_scenes: List, selected_scenes: List):
        """ê·¹ë‹¨ì ì¸ ê²½ìš° ê²½ê³ """
        total_duration = sum(s.duration for s in all_scenes)
        
        if len(selected_scenes) == 0:
            print(f"\nâš ï¸  ê²½ê³ : ì´ë¯¸ì§€ê°€ í•˜ë‚˜ë„ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print(f"  ìµœì†Œ 1-2ê°œëŠ” í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        elif len(selected_scenes) < max(2, total_duration / 180):
            # 3ë¶„ë‹¹ 1ê°œ ë¯¸ë§Œ
            print(f"\nğŸ’¡ ì°¸ê³ : ì´ë¯¸ì§€ê°€ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ({len(selected_scenes)}ê°œ)")
            print(f"  ì½˜í…ì¸ ê°€ ëŒ€í™” ìœ„ì£¼ì¸ ê²½ìš° ì •ìƒì…ë‹ˆë‹¤.")
        
        elif len(selected_scenes) > total_duration / 10:
            # 10ì´ˆë‹¹ 1ê°œ ì´ˆê³¼
            print(f"\nğŸ’¡ ì°¸ê³ : ì´ë¯¸ì§€ê°€ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ({len(selected_scenes)}ê°œ)")
            print(f"  ì½˜í…ì¸ ê°€ ì„¤ëª… ìœ„ì£¼ì¸ ê²½ìš° ì •ìƒì…ë‹ˆë‹¤.")
    
    def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        LangGraph ë…¸ë“œë¡œ ì‹¤í–‰
        
        Args:
            state: {
                "scenes": List[PodcastScene],
                "metadata": PodcastMetadata (optional),
                ...
            }
        
        Returns:
            state with selected_scenes added
        """
        scenes = state.get("scenes", [])
        metadata = state.get("metadata")
        
        if not scenes:
            print("âš ï¸  ì¥ë©´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {**state, "selected_scenes": []}
        
        # ë©”íƒ€ë°ì´í„° ìˆìœ¼ë©´ í™œìš©
        if metadata:
            selected = self.select_scenes_with_metadata(
                scenes=scenes,
                metadata=metadata
            )
        else:
            # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
            print("âš ï¸  ë©”íƒ€ë°ì´í„° ì—†ìŒ - ë…ë¦½ íŒë‹¨ ì‚¬ìš©")
            selected = self.select_scenes(scenes)
        
        return {
            **state,
            "selected_scenes": selected,
            "image_count": len(selected)
        }
    
    def select_scenes(self, scenes: List[PodcastScene]) -> List[PodcastScene]:
        """
        ê¸°ì¡´ ë°©ì‹ (ë©”íƒ€ë°ì´í„° ì—†ì´) - í•˜ìœ„ í˜¸í™˜ì„±
        """
        print(f"\nğŸ¬ ì¥ë©´ ì„ íƒ ì‹œì‘ (ë…ë¦½ íŒë‹¨ ëª¨ë“œ)")
        print("="*80)
        
        selected = []
        
        for scene in scenes:
            # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ íŒë‹¨
            if scene.duration >= 15:  # 15ì´ˆ ì´ìƒ
                scene.image_required = True
                scene.importance = 0.7
                selected.append(scene)
        
        print(f"\nâœ… {len(selected)}ê°œ ì¥ë©´ ì„ íƒ ì™„ë£Œ")
        return selected


# ============================================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================================

def print_selected_scenes(scenes: List[PodcastScene]):
    """ì„ íƒëœ ì¥ë©´ë“¤ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“‹ ì„ íƒëœ ì¥ë©´ ëª©ë¡")
    print("="*80)
    
    for scene in scenes:
        print(f"\nğŸ¬ {scene.scene_id}")
        print(f"  ì‹œê°„: {scene.timestamp_start} ({scene.duration}ì´ˆ)")
        print(f"  í™”ì: {scene.speaker}")
        print(f"  ë‚´ìš©: {scene.text[:80]}...")
        print(f"  ì¤‘ìš”ë„: {scene.importance:.2f}")
        print(f"  ì´ìœ : {scene.context}")


def export_selection_report(
    all_scenes: List[PodcastScene],
    selected_scenes: List[PodcastScene],
    output_path: str
):
    """
    ì„ íƒ ë¦¬í¬íŠ¸ JSON ì €ì¥
    """
    report = {
        "total_scenes": len(all_scenes),
        "selected_scenes": len(selected_scenes),
        "selection_rate": len(selected_scenes) / len(all_scenes) if all_scenes else 0,
        "total_duration": sum(s.duration for s in all_scenes),
        "avg_interval": sum(s.duration for s in all_scenes) / len(selected_scenes) if selected_scenes else 0,
        "scenes": [
            {
                "scene_id": s.scene_id,
                "timestamp_start": s.timestamp_start,
                "duration": s.duration,
                "speaker": s.speaker,
                "text": s.text,
                "image_required": s.image_required,
                "importance": s.importance,
                "chapter_id": getattr(s, 'chapter_id', 'unknown'),
                "content_nature": getattr(s, 'content_nature', 'unknown'),
                "visual_type": getattr(s, 'visual_type', 'none'),
                "reason": s.context
            }
            for s in all_scenes
        ]
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ì„ íƒ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")


if __name__ == "__main__":
    print("Scene Selection Node - ì¥ë©´ ì„ íƒ ë…¸ë“œ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)")
    print("Importí•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”: from scene_selection_node import SceneSelectionNode")
