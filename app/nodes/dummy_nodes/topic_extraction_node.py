"""
í† í”½ ì¶”ì¶œ ë…¸ë“œ (LangGraph)
ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ìƒì„±ìš© í† í”½ ì¶”ì¶œ
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from vertexai.generative_models import GenerativeModel


@dataclass
class ImageTopic:
    """ì´ë¯¸ì§€ ìƒì„±ìš© í† í”½"""
    topic_id: str
    title: str
    description: str
    keywords: List[str]
    style: str  # abstract, technical, illustration, photo, scene
    importance: float  # 0.0 ~ 1.0
    context: Optional[str] = None  # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸


class TopicExtractionNode:
    """ìš”ì•½ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ìƒì„±ìš© í† í”½ ì¶”ì¶œ"""
    
    # í† í”½ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ë²„ì „)
    TOPIC_EXTRACTION_PROMPT = """ë‹¹ì‹ ì€ íŒŸìºìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì œì‘ì„ ìœ„í•œ ì´ë¯¸ì§€ í† í”½ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ì—ì„œ **ë¹„ë””ì˜¤ ì˜¤ë²„ë ˆì´ìš© ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í† í”½**ì„ ì¶”ì¶œí•˜ì„¸ìš”.

**í† í”½ ì¶”ì¶œ ê¸°ì¤€**:
âœ… í¬í•¨: í•µì‹¬ ê°œë…, ì£¼ìš” í¬ì¸íŠ¸, êµ¬ì²´ì  ì‚¬ë¡€, ì‹œê°í™” ê°€ëŠ¥í•œ ì•„ì´ë””ì–´
âŒ ì œì™¸: ì¶”ìƒì  ê°œë…, ì¤‘ë³µ ì•„ì´ë””ì–´, ì‹œê°í™” ë¶ˆê°€ëŠ¥í•œ ë‚´ìš©

**í† í”½ ê°œìˆ˜**: 5-20ê°œ (ë‚´ìš© ê¸¸ì´ì— ë”°ë¼ ì¡°ì ˆ)

**ìŠ¤íƒ€ì¼** (ê° í† í”½ë§ˆë‹¤ í•˜ë‚˜ ì„ íƒ):
1. **abstract** - ê°œë…ì  ì£¼ì œ, ë°°ê²½ ì´ë¯¸ì§€
2. **technical** - ë‹¤ì´ì–´ê·¸ë¨, í”„ë¡œì„¸ìŠ¤, ì›Œí¬í”Œë¡œìš°
3. **illustration** - ì„¤ëª… ê·¸ë˜í”½, ë¹„ìœ ì  í‘œí˜„
4. **photo** - ì‹¤ì œ ì¥ë©´, êµ¬ì²´ì  ì˜ˆì‹œ
5. **scene** - ìŠ¤í† ë¦¬í…”ë§ ì¥ë©´, ìƒí™© ë¬˜ì‚¬

**âš ï¸ description ì‘ì„± ê·œì¹™** (ë§¤ìš° ì¤‘ìš”!):
1. **ì–¸ì–´**: ì˜ì–´ë¡œ ì‘ì„± (ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”)
2. **ì •í™•í•œ ì¥ë©´ ë¬˜ì‚¬**: ì›ë³¸ ë‚´ìš©ì„ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª… (50-100ë‹¨ì–´)
3. **í…ìŠ¤íŠ¸ ì œê±°**: "no text, text-free" í¬í•¨
4. **í•œêµ­ ì»¨í…ìŠ¤íŠ¸**: 
   - âœ… í•œêµ­ ê´€ë ¨ ë‚´ìš©ë§Œ "Korean", "Seoul", "in Korea" ì¶”ê°€
   - âŒ ì„œì–‘ ë¬¸í™”/ì—­ì‚¬/ë™í™”/ì¸ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
   
**í•œêµ­ ì»¨í…ìŠ¤íŠ¸ ì ìš© ê¸°ì¤€**:
- âœ… ì ìš©: "í•œêµ­ ê¸°ì—…", "ì„œìš¸", "K-pop", "í•œêµ­ ì˜ì‚¬", "êµ­ë‚´ ì‚°ì—…"
- âŒ ë¯¸ì ìš©: "ì‹ ë°ë ë¼", "í”¼ì¹´ì†Œ", "íŒŒë¦¬", "ë¥´ë„¤ìƒìŠ¤", "ê·¸ë¦¬ìŠ¤ ì‹ í™”"

**ì˜ˆì‹œ 1 - í•œêµ­ ë‚´ìš©**:
{{
  "title": "AI í—¬ìŠ¤ì¼€ì–´ í˜ì‹ ",
  "description": "Modern Korean medical professional using advanced AI diagnostic system in a Seoul hospital, holographic medical displays showing patient data, clean clinical environment, professional Korean doctor, no text, realistic photography style"
}}

**ì˜ˆì‹œ 2 - ì„œì–‘ ë™í™”**:
{{
  "title": "ì‹ ë°ë ë¼ì˜ ë³€ì‹ ",
  "description": "Cinderella's magical transformation scene at midnight, sparkling fairy godmother magic turning pumpkin into golden carriage, glass slippers glowing, enchanted atmosphere with stars and sparkles, classic European fairy tale setting, no text, illustrated storybook style"
}}

**ì˜ˆì‹œ 3 - ì¶”ìƒ ê°œë…**:
{{
  "title": "AIì˜ ë¯¸ë˜",
  "description": "Abstract futuristic visualization of artificial intelligence concept, flowing neural networks and data streams in blue and purple tones, geometric patterns and glowing nodes, minimalist modern design, no text, digital art style"
}}

**ì¶œë ¥ í˜•ì‹** (JSONë§Œ, ë§ˆí¬ë‹¤ìš´ ì—†ì´):
[
  {{
    "topic_id": "topic_01",
    "title": "í† í”½ ì œëª© (í•œê¸€)",
    "description": "Detailed scene description in English, specific and visual, 50-100 words, no text",
    "keywords": ["keyword1", "keyword2", "keyword3"],
    "style": "abstract",
    "importance": 0.9,
    "context": "ë§¥ë½ ì„¤ëª… (í•œê¸€)"
  }}
]

**ê·œì¹™**:
- topic_idëŠ” ìˆœì°¨ì  ë„˜ë²„ë§ (topic_01, topic_02...)
- descriptionì€ ì›ë³¸ ë‚´ìš©ì„ ì •í™•íˆ ë°˜ì˜, êµ¬ì²´ì ìœ¼ë¡œ
- keywordsëŠ” 5-10ê°œ (ì˜ì–´)
- importanceëŠ” 0.0-1.0 (0.8 ì´ìƒì´ í•µì‹¬ í† í”½)
- styleì€ ë°˜ë“œì‹œ: abstract, technical, illustration, photo, scene ì¤‘ í•˜ë‚˜

**ë¶„ì„í•  ìš”ì•½**:
{summary_content}

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì´ë¯¸ì§€ í† í”½ì„ JSON ë°°ì—´ë¡œ ì¶”ì¶œí•˜ì„¸ìš”. 
ì›ë³¸ ë‚´ìš©ì˜ ë§¥ë½ê³¼ ì˜ë¯¸ë¥¼ ì •í™•íˆ ë³´ì¡´í•˜ì„¸ìš”.
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    def __init__(self, model_name: str = "gemini-2.5-flash"):
        """
        Args:
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸
        """
        self.model = GenerativeModel(model_name)
        self.model_name = model_name
    
    def extract_topics_from_analysis(
        self,
        analysis_result: Dict[str, Any],
        min_topics: int = 5,
        max_topics: int = 20,
        **generation_config
    ) -> List[ImageTopic]:
        """
        ë¶„ì„ ê²°ê³¼ì—ì„œ í† í”½ ì¶”ì¶œ
        
        Args:
            analysis_result: CompleteAnalysis (dict í˜•íƒœ)
            min_topics: ìµœì†Œ í† í”½ ê°œìˆ˜
            max_topics: ìµœëŒ€ í† í”½ ê°œìˆ˜
            **generation_config: Gemini ì„¤ì •
        
        Returns:
            ImageTopic ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ” í† í”½ ì¶”ì¶œ ì‹œì‘")
        
        # 1. í†µí•© ìš”ì•½ ì¶”ì¶œ
        integrated_summary = analysis_result.get('integrated_summary', {})
        
        # ì›ë³¸ ì¶œë ¥ì´ ìˆìœ¼ë©´ ì‚¬ìš© (ë” í’ë¶€í•œ ì •ë³´)
        raw_output = analysis_result.get('metadata', {}).get('raw_output', '')
        
        if raw_output:
            # ì›ë³¸ì—ì„œ í†µí•© ìš”ì•½ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ìˆë‹¤ë©´)
            summary_content = self._extract_summary_section(raw_output)
        else:
            # êµ¬ì¡°í™”ëœ ë°ì´í„° ì‚¬ìš©
            summary_content = self._format_integrated_summary(integrated_summary)
        
        print(f"ğŸ“ ìš”ì•½ ê¸¸ì´: {len(summary_content)} ë¬¸ì")
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.TOPIC_EXTRACTION_PROMPT.format(
            summary_content=summary_content
        )
        
        # 3. Gemini í˜¸ì¶œ
        print("ğŸ¤– Geminië¡œ í† í”½ ì¶”ì¶œ ì¤‘...")
        
        config = {
            "temperature": 0.4,  # ì ë‹¹íˆ ì°½ì˜ì 
            "top_p": 0.9,
            "max_output_tokens": 8192,  # í† í° ì œí•œ ì¦ê°€ (MAX_TOKENS ì—ëŸ¬ ë°©ì§€)
            **generation_config
        }
        
        try:
            response = self.model.generate_content(prompt, generation_config=config)
            
            # ì‘ë‹µ í™•ì¸
            if not response.candidates:
                print("âš ï¸  ì‘ë‹µ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            candidate = response.candidates[0]
            
            # finish_reason í™•ì¸
            if candidate.finish_reason != 1:  # 1 = STOP (ì •ìƒ ì™„ë£Œ)
                finish_reason_map = {
                    2: "MAX_TOKENS",
                    3: "SAFETY",
                    4: "RECITATION",
                    5: "OTHER"
                }
                reason = finish_reason_map.get(candidate.finish_reason, "UNKNOWN")
                print(f"âš ï¸  ë¹„ì •ìƒ ì¢…ë£Œ: {reason}")
                
                if reason == "MAX_TOKENS":
                    print("ğŸ’¡ í† í° ì œí•œ ì´ˆê³¼ - í”„ë¡¬í”„íŠ¸ë¥¼ ì¤„ì´ê±°ë‚˜ max_output_tokensë¥¼ ë” ëŠ˜ë ¤ë³´ì„¸ìš”")
                
                # MAX_TOKENSì¸ ê²½ìš° ë¶€ë¶„ ì‘ë‹µì´ë¼ë„ íŒŒì‹± ì‹œë„
                if reason == "MAX_TOKENS" and hasattr(candidate.content, 'parts'):
                    try:
                        raw_topics = candidate.content.parts[0].text
                        print(f"âš ï¸  ë¶€ë¶„ ì‘ë‹µ ì‚¬ìš© ì‹œë„ ({len(raw_topics)} ë¬¸ì)")
                    except:
                        return []
                else:
                    return []
            else:
                raw_topics = response.text
        
        except Exception as e:
            print(f"âŒ Gemini í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []
        
        print(f"âœ… í† í”½ ì¶”ì¶œ ì™„ë£Œ")
        
        # 4. JSON íŒŒì‹±
        topics = self._parse_topics(raw_topics)
        
        # 5. ê°œìˆ˜ ì œí•œ
        if len(topics) < min_topics:
            print(f"âš ï¸  í† í”½ ê°œìˆ˜ ë¶€ì¡± ({len(topics)} < {min_topics})")
        elif len(topics) > max_topics:
            print(f"âš ï¸  í† í”½ ê°œìˆ˜ ì´ˆê³¼, ìƒìœ„ {max_topics}ê°œë§Œ ì‚¬ìš©")
            topics = sorted(topics, key=lambda t: t.importance, reverse=True)[:max_topics]
        
        print(f"ğŸ“Š ìµœì¢… í† í”½ ê°œìˆ˜: {len(topics)}")
        
        return topics
    
    def _extract_summary_section(self, raw_output: str) -> str:
        """ì›ë³¸ ì¶œë ¥ì—ì„œ í†µí•© ìš”ì•½ ì„¹ì…˜ ì¶”ì¶œ"""
        # "ìµœì¢… í†µí•© ìš”ì•½" ë˜ëŠ” "4. ğŸ“Œ" ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
        markers = [
            "4. ğŸ“Œ ìµœì¢… í†µí•© ìš”ì•½",
            "ìµœì¢… í†µí•© ìš”ì•½",
            "## ìµœì¢… í†µí•© ìš”ì•½",
            "### ìµœì¢… í†µí•© ìš”ì•½"
        ]
        
        for marker in markers:
            if marker in raw_output:
                parts = raw_output.split(marker, 1)
                if len(parts) > 1:
                    return marker + parts[1]
        
        # ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì „ì²´ ë°˜í™˜
        return raw_output
    
    def _format_integrated_summary(self, integrated_summary: Dict) -> str:
        """êµ¬ì¡°í™”ëœ í†µí•© ìš”ì•½ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        sections = integrated_summary.get('sections', [])
        conclusion = integrated_summary.get('conclusion', '')
        
        text_parts = []
        
        for section in sections:
            title = section.get('title', '')
            content = section.get('content', '')
            key_points = section.get('key_points', [])
            
            text_parts.append(f"## {title}")
            text_parts.append(content)
            if key_points:
                text_parts.append("í•µì‹¬ í¬ì¸íŠ¸:")
                for point in key_points:
                    text_parts.append(f"- {point}")
            text_parts.append("")
        
        if conclusion:
            text_parts.append("## ê²°ë¡ ")
            text_parts.append(conclusion)
        
        return "\n".join(text_parts)
    
    def _parse_topics(self, raw_topics: str) -> List[ImageTopic]:
        """JSON ë¬¸ìì—´ì„ ImageTopic ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±"""
        import json
        import re
        
        # JSON ì¶”ì¶œ (markdown ì½”ë“œ ë¸”ë¡ ì œê±°)
        json_text = raw_topics
        
        # ```json ... ``` ì œê±°
        json_text = re.sub(r'```json\s*', '', json_text)
        json_text = re.sub(r'```\s*', '', json_text)
        json_text = json_text.strip()
        
        try:
            topics_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {json_text[:200]}...")
            return []
        
        # ImageTopic ê°ì²´ ìƒì„±
        topics = []
        for data in topics_data:
            try:
                topic = ImageTopic(
                    topic_id=data.get('topic_id', f"topic_{len(topics)+1:02d}"),
                    title=data.get('title', ''),
                    description=data.get('description', ''),
                    keywords=data.get('keywords', []),
                    style=data.get('style', 'abstract'),
                    importance=float(data.get('importance', 0.5)),
                    context=data.get('context')
                )
                topics.append(topic)
            except Exception as e:
                print(f"âš ï¸  í† í”½ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        return topics
    
    def __call__(self, state: dict) -> dict:
        """
        LangGraph ë…¸ë“œ ì‹¤í–‰
        
        Expected state:
            - analysis_result: CompleteAnalysis (dict)
        
        Returns:
            - image_topics: List[ImageTopic]
        """
        analysis_result = state.get("analysis_result")
        
        if not analysis_result:
            raise ValueError("No analysis_result in state")
        
        # CompleteAnalysisê°€ dataclassë©´ dictë¡œ ë³€í™˜
        if hasattr(analysis_result, '__dict__'):
            from dataclasses import asdict
            analysis_result = asdict(analysis_result)
        
        topics = self.extract_topics_from_analysis(analysis_result)
        
        return {
            **state,
            "image_topics": topics
        }


# ============================================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================================

def print_topics_summary(topics: List[ImageTopic]):
    """í† í”½ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ì¶”ì¶œëœ ì´ë¯¸ì§€ í† í”½")
    print("="*80)
    
    print(f"\nì´ {len(topics)}ê°œ í† í”½")
    
    # ìŠ¤íƒ€ì¼ë³„ ë¶„í¬
    style_counts = {}
    for topic in topics:
        style_counts[topic.style] = style_counts.get(topic.style, 0) + 1
    
    print("\nìŠ¤íƒ€ì¼ ë¶„í¬:")
    for style, count in sorted(style_counts.items()):
        print(f"  {style}: {count}ê°œ")
    
    # í† í”½ ëª©ë¡
    print("\n" + "-"*80)
    print("í† í”½ ìƒì„¸")
    print("-"*80)
    
    for i, topic in enumerate(topics, 1):
        print(f"\n[{i}] {topic.topic_id}")
        print(f"  ì œëª©: {topic.title}")
        print(f"  ìŠ¤íƒ€ì¼: {topic.style}")
        print(f"  ì¤‘ìš”ë„: {topic.importance:.2f}")
        print(f"  ì„¤ëª…: {topic.description[:100]}...")
        print(f"  í‚¤ì›Œë“œ: {', '.join(topic.keywords[:5])}")
        if topic.context:
            print(f"  ì»¨í…ìŠ¤íŠ¸: {topic.context}")


def save_topics_to_json(topics: List[ImageTopic], output_path: str):
    """í† í”½ì„ JSONìœ¼ë¡œ ì €ì¥"""
    import json
    from dataclasses import asdict
    
    topics_dict = [asdict(topic) for topic in topics]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(topics_dict, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ í† í”½ ì €ì¥: {output_path}")