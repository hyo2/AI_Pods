"""
ë¬¸ì„œ ë¶„ì„ ë…¸ë“œ (LangGraph)
Phase 1: í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹¨ì¼/ë©€í‹° ì†ŒìŠ¤ ë¶„ì„
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from vertexai.generative_models import GenerativeModel
import json


@dataclass
class SourceDocument:
    """ì†ŒìŠ¤ ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    content: str
    metadata: Optional[Dict[str, Any]] = None
    doc_type: Optional[str] = None  # "text", "pdf", "url" ë“±


@dataclass
class DocumentAnalysis:
    """ë¬¸ì„œ ë¶„ì„ ê²°ê³¼"""
    source_id: str
    doc_type: str
    core_topic: str
    detailed_summary: str
    key_sentences: List[str]
    keywords: List[str]
    raw_analysis: str


@dataclass
class RelationshipAnalysis:
    """ë¬¸ì„œ ê°„ ê´€ê³„ ë¶„ì„"""
    common_themes: List[str]
    complementary_content: str
    differences: str
    contradictions: Optional[str]
    mega_topic: str
    raw_analysis: str


@dataclass
class ClusteringResult:
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼"""
    topic_clusters: List[Dict[str, Any]]
    sub_clusters: List[Dict[str, Any]]
    raw_analysis: str


@dataclass
class IntegratedSummary:
    """ìµœì¢… í†µí•© ìš”ì•½"""
    sections: List[Dict[str, Any]]
    conclusion: str
    raw_analysis: str


@dataclass
class CompleteAnalysis:
    """ì „ì²´ ë¶„ì„ ê²°ê³¼"""
    individual_analyses: List[DocumentAnalysis]
    relationship_analysis: Optional[RelationshipAnalysis]
    clustering: ClusteringResult
    integrated_summary: IntegratedSummary
    metadata: Dict[str, Any]


class DocumentAnalysisNode:
    """ë¬¸ì„œ ë¶„ì„ ë…¸ë“œ"""
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    ANALYSIS_PROMPT = """ë‹¹ì‹ ì—ê²Œ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œê°€ ì…ë ¥ë©ë‹ˆë‹¤. ë¬¸ì„œì˜ í˜•ì‹(PDF, ì›¹í˜ì´ì§€, ê¸°ì‚¬, ë…¸íŠ¸, ë³´ê³ ì„œ ë“±)ì€ ì„œë¡œ ë‹¤ë¥´ë©°, ê¸¸ì´ì™€ ì •ë³´ëŸ‰ ì—­ì‹œ ì œê°ê°ì…ë‹ˆë‹¤.
ì´ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬, "ì „ë¬¸ íŒŸìºìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë² ì´ìŠ¤ ì •ë³´ êµ¬ì¡°"ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
-----------------------------------------
[í•µì‹¬ ëª©í‘œ]
ë¬¸ì„œ ì „ì²´ë¥¼ ë‹¤ìŒ ë„¤ ë‹¨ê³„ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”:
1) ë¬¸ì„œë³„ ê°œë³„ ë¶„ì„  
2) ë¬¸ì„œ ê°„ ê´€ê³„ ë¶„ì„  
3) ì •ë³´ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ì „ì²´ êµ¬ì¡°í™”  
4) ìµœì¢… í†µí•© ìš”ì•½ ìƒì„±
ë¬¸ì„œ ê°œìˆ˜ê°€ ë§ì•„ë„ ì¼ê´€ëœ ì¶œë ¥ì´ ìœ ì§€ë˜ë„ë¡ í•˜ì„¸ìš”.
-----------------------------------------
[ì¶œë ¥ í˜•ì‹]
1. ğŸ“Œ ì†ŒìŠ¤ë³„ í•µì‹¬ ë¶„ì„ (ë¬¸ì„œ ê°œìˆ˜ë§Œí¼ ë°˜ë³µ)
   - ë¬¸ì„œ ID: (ë¬¸ì„œ ëª…ì¹­ ë˜ëŠ” ë²ˆí˜¸)
   - ë¬¸ì„œ ìœ í˜• ì¶”ì •(PDF/ì›¹/ë³´ê³ ì„œ ë“±)
   - **í•µì‹¬ ì£¼ì œ í•œ ì¤„ ìš”ì•½**
   - ìƒì„¸ ìš”ì•½ (5~7ë¬¸ì¥)
   - ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥ 3ê°œ ë°œì·Œ
   - ë¬¸ì„œ ë‚´ í•µì‹¬ í‚¤ì›Œë“œ(5~10ê°œ)
---
2. ğŸ“Œ ì†ŒìŠ¤ ê°„ ê´€ê³„ ë¶„ì„
   - ë¬¸ì„œë“¤ ì‚¬ì´ì˜ ê³µí†µ ì£¼ì œ
   - ì„œë¡œ ë³´ì™„í•˜ëŠ” ë‚´ìš©
   - ê´€ì Â·ì£¼ì¥Â·ë°ì´í„° ì°¨ì´
   - ëª¨ìˆœ ë˜ëŠ” ì¶©ëŒ ì§€ì (ìˆì„ ê²½ìš°)
   - ì „ì²´ ë¬¸ì„œë“¤ì´ í•¨ê»˜ í˜•ì„±í•˜ëŠ” "ë©”ê°€ ì£¼ì œ"
---
3. ğŸ“Œ ì „ì²´ ë¬¸ì„œ í†µí•© í´ëŸ¬ìŠ¤í„°ë§  
ëª¨ë“  ì†ŒìŠ¤ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ "ìë™ ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§"ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
ì•„ë˜ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ìœ ì§€í•˜ì„¸ìš”:
### 3-1. í† í”½ í´ëŸ¬ìŠ¤í„°(ìµœìƒìœ„ ê·¸ë£¹)
- í´ëŸ¬ìŠ¤í„° ì´ë¦„:
- ì„¤ëª…: (1~2ë¬¸ì¥)
- í¬í•¨ëœ ë¬¸ì„œ ë˜ëŠ” ì„¹ì…˜:
- ì´ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¤‘ìš”í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3~5ê°œ
### 3-2. ì„œë¸Œ í´ëŸ¬ìŠ¤í„°(í•„ìš”í•œ ë§Œí¼)
- í•˜ìœ„ ì£¼ì œ ì´ë¦„:
- í•µì‹¬ ë‚´ìš© 3~5ì¤„
- ì‚¬ìš©ìì—ê²Œ ì˜ë¯¸ ìˆëŠ” ì´ìœ 
(ë¬¸ì„œê°€ ë§ìœ¼ë©´ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìë™ ì¡°ì ˆ)
---
4. ğŸ“Œ ìµœì¢… í†µí•© ìš”ì•½ (ì‚¬ìš©ì ì²­ì·¨/í•™ìŠµìš©)
ì•„ë˜ ì›ì¹™ì„ ì§€ì¼œ ì‘ì„±í•˜ì„¸ìš”:
- í•˜ë‚˜ì˜ ì¼ê´€ëœ ë¬¸ì„œì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ê²Œ ì‘ì„±
- ì •ë³´ëŸ‰ì€ í’ë¶€í•˜ë˜ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ
- ì´ì•¼ê¸° íë¦„ì´ ìˆë„ë¡ êµ¬ì„±
- ì¤‘ìš”í•œ ê°œë…, íŠ¸ë Œë“œ, ì¸ì‚¬ì´íŠ¸ëŠ” ë¹ ì§ì—†ì´ í¬í•¨
êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ì´:
1) ì „ì²´ ë‚´ìš©ì„ 4~6ê°œ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆˆ ë…¼ë¦¬ì  êµ¬ì„±  
2) ê° ì„¹ì…˜ì€ 'ì£¼ì œ í•œ ë¬¸ì¥ â†’ ìƒì„¸ ì„¤ëª… â†’ í•µì‹¬ í¬ì¸íŠ¸ 3ê°œ'ë¡œ êµ¬ì„±  
3) í†µí•© ê²°ë¡ (ì‚¬ìš©ìê°€ ì–»ì–´ê°ˆ í•µì‹¬ ë©”ì‹œì§€)
-----------------------------------------
[ì¶”ê°€ ì¡°ê±´]
- ë¬¸ì„œê°€ 1ê°œì—¬ë„ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.  
- ë¬¸ì„œê°€ 10ê°œ ì´ìƒì´ì–´ë„ ìš”ì•½ í’ˆì§ˆì„ ìœ ì§€í•˜ì„¸ìš”.  
- ì¤‘ë³µëœ ì •ë³´ëŠ” í†µí•©í•˜ê³ , ì‹œê° ì°¨ì´ëŠ” ë¶„ëª…í•˜ê²Œ ë³´ì—¬ì£¼ì„¸ìš”.  
- í‘œë©´ì  ìš”ì•½ì´ ì•„ë‹ˆë¼, "êµ¬ì¡°ì  ì¬í•´ì„"ì„ ëª©í‘œë¡œ í•˜ì„¸ìš”.
- ë¬¸ì„œì˜ ê¸¸ì´Â·í’ˆì§ˆÂ·í˜•ì‹ì´ ì„œë¡œ ë‹¬ë¼ë„ ì¼ê´€ëœ ì¶œë ¥ ì œê³µ.
-----------------------------------------
ì´ì œ ìœ„ êµ¬ì¡°ì— ë”°ë¼ ì…ë ¥ëœ ëª¨ë“  ì†ŒìŠ¤ë¥¼ í†µí•© ë¶„ì„í•˜ì„¸ìš”.

[ì…ë ¥ ë¬¸ì„œë“¤]
{documents}

ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”."""
    
    def __init__(self, model_name: str = "gemini-2.0-flash-exp"):
        """
        Args:
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸
        """
        self.model = GenerativeModel(model_name)
        self.model_name = model_name
    
    def format_documents(self, sources: List[SourceDocument]) -> str:
        """ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        formatted = []
        
        for i, source in enumerate(sources, 1):
            doc_info = f"""
=== ë¬¸ì„œ {i} ===
ë¬¸ì„œ ID: {source.id}
ë¬¸ì„œ ìœ í˜•: {source.doc_type or "í…ìŠ¤íŠ¸"}
ë‚´ìš©:
{source.content}
{'='*50}
"""
            formatted.append(doc_info)
        
        return "\n".join(formatted)
    
    def analyze_documents(
        self, 
        sources: List[SourceDocument],
        **generation_config
    ) -> CompleteAnalysis:
        """
        ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰
        
        Args:
            sources: ë¶„ì„í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            **generation_config: Gemini ìƒì„± ì„¤ì •
        
        Returns:
            ì™„ì „í•œ ë¶„ì„ ê²°ê³¼
        """
        print(f"\nğŸ“Š ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {len(sources)}ê°œ ë¬¸ì„œ")
        
        # 1. í”„ë¡¬í”„íŠ¸ ìƒì„±
        documents_text = self.format_documents(sources)
        prompt = self.ANALYSIS_PROMPT.format(documents=documents_text)
        
        # 2. Gemini í˜¸ì¶œ
        print("ğŸ¤– Gemini ë¶„ì„ ì¤‘...")
        
        config = {
            "temperature": 0.3,  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„
            "top_p": 0.95,
            "max_output_tokens": 8192,
            **generation_config
        }
        
        response = self.model.generate_content(
            prompt,
            generation_config=config
        )
        
        raw_text = response.text
        print(f"âœ… ë¶„ì„ ì™„ë£Œ ({len(raw_text)} ë¬¸ì)")
        
        # 3. ê²°ê³¼ íŒŒì‹±
        print("ğŸ“ ê²°ê³¼ íŒŒì‹± ì¤‘...")
        parsed = self._parse_analysis(raw_text, sources)
        
        return parsed
    
    def _parse_analysis(
        self, 
        raw_text: str, 
        sources: List[SourceDocument]
    ) -> CompleteAnalysis:
        """
        Gemini ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ íŒŒì‹±
        
        Note: ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        í˜„ì¬ëŠ” ê¸°ë³¸ êµ¬ì¡°ë§Œ ì œê³µ
        """
        # TODO: ì‹¤ì œ íŒŒì‹± ë¡œì§ êµ¬í˜„
        # ì§€ê¸ˆì€ ê¸°ë³¸ êµ¬ì¡°ë§Œ ë°˜í™˜
        
        # ê°œë³„ ë¶„ì„ (ì„ì‹œ)
        individual_analyses = []
        for source in sources:
            individual_analyses.append(DocumentAnalysis(
                source_id=source.id,
                doc_type=source.doc_type or "text",
                core_topic="[íŒŒì‹± í•„ìš”]",
                detailed_summary="[íŒŒì‹± í•„ìš”]",
                key_sentences=["[íŒŒì‹± í•„ìš”]"],
                keywords=["[íŒŒì‹± í•„ìš”]"],
                raw_analysis=raw_text
            ))
        
        # ê´€ê³„ ë¶„ì„ (ì„ì‹œ)
        relationship_analysis = None
        if len(sources) > 1:
            relationship_analysis = RelationshipAnalysis(
                common_themes=["[íŒŒì‹± í•„ìš”]"],
                complementary_content="[íŒŒì‹± í•„ìš”]",
                differences="[íŒŒì‹± í•„ìš”]",
                contradictions=None,
                mega_topic="[íŒŒì‹± í•„ìš”]",
                raw_analysis=raw_text
            )
        
        # í´ëŸ¬ìŠ¤í„°ë§ (ì„ì‹œ)
        clustering = ClusteringResult(
            topic_clusters=[{
                "name": "[íŒŒì‹± í•„ìš”]",
                "description": "[íŒŒì‹± í•„ìš”]",
                "documents": [],
                "insights": []
            }],
            sub_clusters=[],
            raw_analysis=raw_text
        )
        
        # í†µí•© ìš”ì•½ (ì„ì‹œ)
        integrated_summary = IntegratedSummary(
            sections=[{
                "title": "[íŒŒì‹± í•„ìš”]",
                "content": "[íŒŒì‹± í•„ìš”]",
                "key_points": []
            }],
            conclusion="[íŒŒì‹± í•„ìš”]",
            raw_analysis=raw_text
        )
        
        return CompleteAnalysis(
            individual_analyses=individual_analyses,
            relationship_analysis=relationship_analysis,
            clustering=clustering,
            integrated_summary=integrated_summary,
            metadata={
                "source_count": len(sources),
                "model": self.model_name,
                "raw_output_length": len(raw_text),
                "raw_output": raw_text  # ì „ì²´ ì›ë³¸ ì €ì¥
            }
        )
    
    def __call__(self, state: dict) -> dict:
        """
        LangGraph ë…¸ë“œ ì‹¤í–‰
        
        Expected state:
            - sources: List[SourceDocument]
        
        Returns:
            - analysis_result: CompleteAnalysis
        """
        sources = state.get("sources", [])
        
        if not sources:
            raise ValueError("No sources provided for analysis")
        
        result = self.analyze_documents(sources)
        
        return {
            **state,
            "analysis_result": result
        }


# ============================================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================================

def create_source_from_text(text: str, doc_id: str = None) -> SourceDocument:
    """í…ìŠ¤íŠ¸ì—ì„œ SourceDocument ìƒì„±"""
    import hashlib
    
    if doc_id is None:
        doc_id = hashlib.md5(text[:100].encode()).hexdigest()[:8]
    
    return SourceDocument(
        id=doc_id,
        content=text,
        doc_type="text"
    )


def create_sources_from_texts(texts: List[str]) -> List[SourceDocument]:
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì—ì„œ SourceDocument ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
    return [
        create_source_from_text(text, f"doc_{i+1}")
        for i, text in enumerate(texts)
    ]


def save_analysis_to_json(analysis: CompleteAnalysis, output_path: str):
    """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    import json
    from dataclasses import asdict
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(asdict(analysis), f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")


def print_analysis_summary(analysis: CompleteAnalysis):
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    print(f"\nğŸ“„ ë¶„ì„ëœ ë¬¸ì„œ ìˆ˜: {len(analysis.individual_analyses)}")
    print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {analysis.metadata.get('model')}")
    print(f"ğŸ“ ì›ë³¸ ì¶œë ¥ ê¸¸ì´: {analysis.metadata.get('raw_output_length')} ë¬¸ì")
    
    print("\n" + "-"*80)
    print("ğŸ“Œ ê°œë³„ ë¬¸ì„œ ë¶„ì„")
    print("-"*80)
    for i, doc_analysis in enumerate(analysis.individual_analyses, 1):
        print(f"\n[ë¬¸ì„œ {i}] {doc_analysis.source_id}")
        print(f"  ìœ í˜•: {doc_analysis.doc_type}")
        print(f"  í•µì‹¬ ì£¼ì œ: {doc_analysis.core_topic}")
    
    if analysis.relationship_analysis:
        print("\n" + "-"*80)
        print("ğŸ“Œ ë¬¸ì„œ ê°„ ê´€ê³„")
        print("-"*80)
        print(f"ë©”ê°€ ì£¼ì œ: {analysis.relationship_analysis.mega_topic}")
    
    print("\n" + "-"*80)
    print("ğŸ“Œ í´ëŸ¬ìŠ¤í„°ë§")
    print("-"*80)
    print(f"í† í”½ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(analysis.clustering.topic_clusters)}")
    print(f"ì„œë¸Œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(analysis.clustering.sub_clusters)}")
    
    print("\n" + "-"*80)
    print("ğŸ“Œ í†µí•© ìš”ì•½")
    print("-"*80)
    print(f"ì„¹ì…˜ ìˆ˜: {len(analysis.integrated_summary.sections)}")
    
    print("\n" + "="*80)
    print("ğŸ’¡ ì „ì²´ ì›ë³¸ ì¶œë ¥ì€ metadata['raw_output']ì—ì„œ í™•ì¸í•˜ì„¸ìš”")
    print("="*80)
